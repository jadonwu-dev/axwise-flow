"""
Customer Research API v3 Simplified - Production-Ready Implementation.

This implementation combines the best features of V1, V2, and V3 while maintaining
stability and performance. It provides:

- All V3 enhanced features with V1/V2 stability patterns
- Unified LLM client with proper error handling
- Smart caching and performance optimization
- Production-ready monitoring and metrics
- Extensible architecture for future enhancements

Key improvements over V3:
- Single LLM client instance (no duplication)
- Sequential processing with smart caching (no parallel complexity)
- Proper environment variable handling
- Bounded collections and memory management
- Simplified error handling with clear fallback paths
- Request-scoped services (no global state)
"""

import logging
import time
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field, ValidationError
from sqlalchemy.orm import Session

from backend.database import get_db
# Optional imports - will be imported when needed to avoid dependency issues
# from backend.services.research_session_service import ResearchSessionService
# from backend.models.research_session import ResearchSessionCreate, ResearchSessionUpdate
# from backend.utils.research_validation import validate_research_request, ValidationError as ResearchValidationError
# from backend.utils.research_error_handler import ErrorHandler, with_retry, with_timeout

logger = logging.getLogger(__name__)

# Router configuration
router = APIRouter(
    prefix="/api/research/v3-simple",
    tags=["Customer Research V3 Simplified"],
    responses={
        404: {"description": "Not found"},
        500: {"description": "Internal server error"}
    }
)


@dataclass
class SimplifiedConfig:
    """Simplified configuration for V3 research service."""

    # Feature flags
    enable_industry_analysis: bool = True
    enable_stakeholder_detection: bool = True
    enable_enhanced_context: bool = True
    enable_conversation_flow: bool = True
    enable_thinking_process: bool = True

    # Performance settings
    enable_caching: bool = True
    cache_ttl_seconds: int = 300  # 5 minutes
    request_timeout_seconds: int = 30

    # Quality settings
    min_confidence_threshold: float = 0.6
    enable_quality_checks: bool = True

    # Fallback settings
    enable_v1_fallback: bool = False  # Disable V1 fallback to force V3 to work
    max_retries: int = 2

    # Memory management
    max_thinking_steps: int = 20
    max_metrics_history: int = 100


@dataclass
class RequestMetrics:
    """Metrics for a single request."""

    request_id: str
    start_time: float = field(default_factory=time.time)
    end_time: float = 0.0

    # Component timings (ms)
    context_analysis_ms: int = 0
    intent_analysis_ms: int = 0
    business_validation_ms: int = 0
    industry_analysis_ms: int = 0
    stakeholder_detection_ms: int = 0
    conversation_flow_ms: int = 0
    response_generation_ms: int = 0

    # Quality metrics
    confidence_scores: Dict[str, float] = field(default_factory=dict)
    cache_hits: int = 0
    cache_misses: int = 0

    # Error tracking
    errors_encountered: List[str] = field(default_factory=list)
    fallback_used: bool = False

    @property
    def total_duration_ms(self) -> int:
        """Calculate total duration in milliseconds."""
        if self.end_time > 0:
            return int((self.end_time - self.start_time) * 1000)
        return int((time.time() - self.start_time) * 1000)

    @property
    def success_rate(self) -> float:
        """Calculate success rate based on errors."""
        total_operations = 7  # Number of main operations
        failed_operations = len(self.errors_encountered)
        return max(0.0, (total_operations - failed_operations) / total_operations)


class Message(BaseModel):
    id: str
    content: str
    role: str  # 'user' or 'assistant'
    timestamp: str
    metadata: Optional[Dict[str, Any]] = None


class ResearchContext(BaseModel):
    businessIdea: Optional[str] = None
    targetCustomer: Optional[str] = None
    problem: Optional[str] = None
    stage: Optional[str] = None
    questionsGenerated: Optional[bool] = None
    multiStakeholderConsidered: Optional[bool] = None
    multiStakeholderDetected: Optional[bool] = None
    detectedStakeholders: Optional[Dict[str, Any]] = None


class ChatRequest(BaseModel):
    messages: List[Message]
    input: str
    context: Optional[ResearchContext] = None
    session_id: Optional[str] = None
    user_id: Optional[str] = None

    # V3 Simple options
    enable_enhanced_analysis: bool = Field(default=True, description="Enable enhanced analysis features")
    enable_thinking_process: bool = Field(default=True, description="Enable thinking process tracking")


class ChatResponse(BaseModel):
    content: str
    metadata: Optional[Dict[str, Any]] = None
    questions: Optional[Any] = None  # Support any format - simple, comprehensive, or custom
    session_id: Optional[str] = None
    thinking_process: Optional[List[Dict[str, Any]]] = None
    performance_metrics: Optional[Dict[str, Any]] = None
    api_version: str = Field(default="v3-simple")


class HealthResponse(BaseModel):
    status: str
    version: str
    features: List[str]
    performance: Dict[str, Any]
    timestamp: str


class GenerateQuestionsRequest(BaseModel):
    context: ResearchContext
    conversationHistory: List[Message]


class ResearchQuestions(BaseModel):
    problemDiscovery: List[str]
    solutionValidation: List[str]
    followUp: List[str]


class SimplifiedResearchService:
    """
    Simplified research service that provides all V3 features with V1/V2 stability.

    This service is designed to be:
    - Request-scoped (no global state)
    - Memory efficient (bounded collections)
    - Error resilient (clear fallback paths)
    - Performance optimized (smart caching)
    - Production ready (proper monitoring)
    """

    def __init__(self, config: Optional[SimplifiedConfig] = None):
        """Initialize the simplified research service."""

        self.config = config or SimplifiedConfig()
        self.request_id = f"req_{int(time.time() * 1000)}_{id(self)}"

        # Initialize metrics for this request
        self.metrics = RequestMetrics(request_id=self.request_id)

        # Initialize thinking process tracking
        self.thinking_steps: List[Dict[str, Any]] = []

        # Initialize cache for this request
        self.request_cache: Dict[str, Any] = {}

        # Initialize LLM interaction tracking for raw content capture
        self.llm_interactions: List[Dict[str, Any]] = []

        # Initialize LLM client (reuse from V1/V2 proven patterns)
        self._llm_client = None

        # Store instance in global registry for progressive updates
        SimplifiedResearchService._active_instances[self.request_id] = self
        logger.debug(f"Added instance {self.request_id} to registry. Total active instances: {len(SimplifiedResearchService._active_instances)}")

        logger.info(f"Initialized SimplifiedResearchService {self.request_id}")

    # Class-level registry for active instances (for progressive updates)
    _active_instances: Dict[str, 'SimplifiedResearchService'] = {}

    def _get_llm_client(self):
        """Get or create LLM client using proven V1/V2 patterns."""
        if self._llm_client is None:
            # Import the proven LLM service factory from V1/V2
            from backend.services.llm import LLMServiceFactory
            self._llm_client = LLMServiceFactory.create("gemini")
        return self._llm_client

    def _add_thinking_step(self, step: str, status: str = "in_progress", details: str = "", duration_ms: int = 0):
        """Add or update a thinking step with memory management."""
        try:
            # Check if this step already exists (to update instead of duplicate)
            existing_step_index = None
            for i, existing_step in enumerate(self.thinking_steps):
                if existing_step["step"] == step:
                    existing_step_index = i
                    break

            thinking_step = {
                "step": step,
                "status": status,
                "details": details,
                "duration_ms": duration_ms,
                "timestamp": int(time.time() * 1000)
            }

            if existing_step_index is not None:
                # Update existing step
                self.thinking_steps[existing_step_index] = thinking_step
            else:
                # Add new step
                self.thinking_steps.append(thinking_step)

            # Memory management: keep only recent steps
            if len(self.thinking_steps) > self.config.max_thinking_steps:
                self.thinking_steps = self.thinking_steps[-self.config.max_thinking_steps:]

            logger.debug(f"{'Updated' if existing_step_index is not None else 'Added'} thinking step: {step}")
        except Exception as e:
            logger.warning(f"Failed to add thinking step: {e}")

    def _capture_llm_interaction(self, operation_name: str, prompt: str, response: str, metadata: Dict[str, Any] = None):
        """Capture raw LLM interactions for transparent thinking process."""
        try:
            interaction = {
                "operation": operation_name,
                "timestamp": int(time.time() * 1000),
                "prompt": prompt,
                "response": response,
                "metadata": metadata or {}
            }
            self.llm_interactions.append(interaction)

            # Memory management: keep only recent interactions
            if len(self.llm_interactions) > 20:  # Keep last 20 interactions
                self.llm_interactions = self.llm_interactions[-20:]

            logger.debug(f"Captured LLM interaction for {operation_name}")
        except Exception as e:
            logger.warning(f"Failed to capture LLM interaction: {e}")

    def get_current_thinking_steps(self) -> List[Dict[str, Any]]:
        """Get current thinking steps for progressive updates."""
        return self.thinking_steps.copy()

    def cleanup(self):
        """Clean up instance from registry."""
        if self.request_id in SimplifiedResearchService._active_instances:
            del SimplifiedResearchService._active_instances[self.request_id]
            logger.debug(f"Removed instance {self.request_id} from registry. Remaining active instances: {len(SimplifiedResearchService._active_instances)}")
        else:
            logger.warning(f"Instance {self.request_id} not found in registry during cleanup")

    def _get_operation_description(self, operation_name: str, phase: str, result: Any = None, duration_ms: int = None) -> str:
        """Generate properly formatted, readable descriptions for thinking process steps with raw LLM content."""

        if phase == "starting":
            descriptions = {
                "Context Analysis": "ðŸ” CONTEXT EXTRACTION\n\nAnalyzing conversation to extract business idea, target customers, and problem statement...",
                "Intent Analysis": "ðŸŽ¯ INTENT DETECTION\n\nDetermining user's current intent and conversation stage...",
                "Business Validation": "âœ… READINESS ASSESSMENT\n\nEvaluating business concept completeness for research question generation...",
                "Industry Analysis": "ðŸ¢ INDUSTRY CLASSIFICATION\n\nClassifying business into relevant industry categories...",
                "Stakeholder Detection": "ðŸ‘¥ STAKEHOLDER MAPPING\n\nIdentifying key stakeholders in the business ecosystem...",
                "Conversation Flow": "ðŸ”„ FLOW ANALYSIS\n\nAnalyzing conversation progression and determining next steps...",
                "Response Generation": "ðŸ’¬ RESPONSE CREATION\n\nGenerating contextual response and suggestions..."
            }
            return descriptions.get(operation_name, f"ðŸ”§ {operation_name.upper()}\n\nStarting {operation_name.lower()}...")

        elif phase == "completed" and result:
            # Find the most recent LLM interaction for this operation
            llm_content = ""
            for interaction in reversed(self.llm_interactions):
                if interaction["operation"] == operation_name:
                    # Format the prompt with proper line breaks and logical truncation
                    prompt = self._format_prompt_content(interaction["prompt"])

                    # Format the response with proper JSON formatting if it's JSON
                    response = self._format_response_content(interaction["response"])

                    # Format parsed result in a readable way
                    parsed_result = self._format_parsed_result(operation_name, result)

                    llm_content = f"""

ðŸ“ LLM PROMPT:
{prompt}


ðŸ¤– LLM RESPONSE:
{response}


ðŸ“Š PARSED RESULT:
{parsed_result}"""
                    break

            # If no LLM interaction found, show formatted parsed result only
            if not llm_content:
                parsed_result = self._format_parsed_result(operation_name, result)
                llm_content = f"""

ðŸ“Š OPERATION RESULT:
{parsed_result}"""

            return f"âœ… {operation_name.upper()} COMPLETED ({duration_ms}ms){llm_content}"

        # Fallback for any unhandled cases
        return f"ðŸ”§ {operation_name} {'completed' if duration_ms else 'in progress'}{'(' + str(duration_ms) + 'ms)' if duration_ms else '...'}"

    def _format_prompt_content(self, prompt: str) -> str:
        """Format prompt content with proper truncation and line breaks."""
        if len(prompt) <= 500:
            return prompt

        # Find logical truncation points
        truncate_at = -1

        # Try to find end of a complete section (double newline)
        for i in range(300, min(500, len(prompt))):
            if prompt[i:i+2] == '\n\n':
                truncate_at = i
                break

        # If no section break, try end of sentence
        if truncate_at == -1:
            for i in range(400, min(500, len(prompt))):
                if prompt[i:i+2] in ['. ', '.\n']:
                    truncate_at = i + 1
                    break

        # If no sentence break, use hard limit
        if truncate_at == -1:
            truncate_at = 450

        return prompt[:truncate_at] + "\n\n[... content truncated for readability ...]"

    def _format_response_content(self, response: str) -> str:
        """Format response content with proper JSON formatting and truncation."""
        try:
            import json
            # Try to parse and pretty-print JSON
            if response.strip().startswith('{') or response.strip().startswith('['):
                parsed_json = json.loads(response)
                formatted_json = json.dumps(parsed_json, indent=2, ensure_ascii=False)

                # If formatted JSON is too long, truncate intelligently
                if len(formatted_json) > 800:
                    lines = formatted_json.split('\n')
                    truncated_lines = []
                    char_count = 0

                    for line in lines:
                        if char_count + len(line) > 600:
                            truncated_lines.append("  ...")
                            truncated_lines.append("}")
                            break
                        truncated_lines.append(line)
                        char_count += len(line)

                    return '\n'.join(truncated_lines)

                return formatted_json
        except:
            pass

        # If not JSON or parsing failed, format as regular text
        if len(response) <= 600:
            return response

        # Find logical truncation point for text
        truncate_at = response.find('\n', 500)
        if truncate_at == -1:
            truncate_at = response.find('. ', 500)
        if truncate_at == -1:
            truncate_at = 550

        return response[:truncate_at] + "\n\n[... content truncated for readability ...]"

    def _format_parsed_result(self, operation_name: str, result: Any) -> str:
        """Format the parsed result in a readable way based on operation type."""

        if operation_name == "Context Analysis" and isinstance(result, dict):
            # Get values with explicit None handling
            business_idea = result.get('businessIdea') or result.get('business_idea')
            target_customer = result.get('targetCustomer') or result.get('target_customer')
            problem = result.get('problem')

            # Convert None values to strings explicitly
            business_idea = 'Not specified' if business_idea is None else str(business_idea)
            target_customer = 'Not specified' if target_customer is None else str(target_customer)
            problem = 'Not specified' if problem is None else str(problem)

            # Truncate long values for readability
            business_idea = self._truncate_text(business_idea, 80)
            target_customer = self._truncate_text(target_customer, 60)
            problem = self._truncate_text(problem, 100)

            return f"â€¢ Business Idea: {business_idea}\nâ€¢ Target Customer: {target_customer}\nâ€¢ Problem: {problem}"

        elif operation_name == "Intent Analysis" and isinstance(result, dict):
            intent = result.get('intent')
            confidence = result.get('confidence')
            reasoning = result.get('reasoning')
            next_action = result.get('next_action')

            # Convert None values to strings explicitly
            intent = 'unknown' if intent is None else str(intent)
            confidence = 0 if confidence is None else confidence
            reasoning = 'No reasoning provided' if reasoning is None else str(reasoning)
            next_action = 'Continue conversation' if next_action is None else str(next_action)

            reasoning = self._truncate_text(reasoning, 120)
            next_action = self._truncate_text(next_action, 80)

            return f"â€¢ Intent: {intent}\nâ€¢ Confidence: {confidence:.1%}\nâ€¢ Reasoning: {reasoning}\nâ€¢ Next Action: {next_action}"

        elif operation_name == "Business Validation" and isinstance(result, dict):
            ready = result.get('ready_for_questions', False)
            confidence = result.get('confidence')
            reasoning = result.get('reasoning')
            missing = result.get('missing_elements')
            quality = result.get('conversation_quality')

            # Convert None values to appropriate defaults
            confidence = 0 if confidence is None else confidence
            reasoning = 'No reasoning provided' if reasoning is None else str(reasoning)
            missing = [] if missing is None else missing
            quality = 'unknown' if quality is None else str(quality)

            reasoning = self._truncate_text(reasoning, 150)
            missing_text = ', '.join(str(m) for m in missing[:3]) if missing else 'None'
            if len(missing) > 3:
                missing_text += f" (+{len(missing)-3} more)"

            return f"â€¢ Ready for Questions: {'Yes' if ready else 'No'}\nâ€¢ Confidence: {confidence:.1%}\nâ€¢ Quality: {quality}\nâ€¢ Missing Elements: {missing_text}\nâ€¢ Reasoning: {reasoning}"

        elif operation_name == "Industry Analysis" and isinstance(result, dict):
            industry = result.get('industry', 'Unknown')
            confidence = result.get('confidence', 0)
            reasoning = result.get('reasoning', 'No reasoning provided')
            sub_categories = result.get('sub_categories', [])

            reasoning = self._truncate_text(reasoning, 120)
            sub_cats_text = ', '.join(sub_categories[:3]) if sub_categories else 'None'
            if len(sub_categories) > 3:
                sub_cats_text += f" (+{len(sub_categories)-3} more)"

            return f"â€¢ Industry: {industry}\nâ€¢ Confidence: {confidence:.1%}\nâ€¢ Sub-categories: {sub_cats_text}\nâ€¢ Reasoning: {reasoning}"

        elif operation_name == "Stakeholder Detection" and isinstance(result, dict):
            stakeholders = result.get('stakeholders', [])
            confidence = result.get('confidence', 0)

            stakeholder_names = [s.get('name', 'Unknown') for s in stakeholders[:4]]
            stakeholder_text = ', '.join(stakeholder_names) if stakeholder_names else 'None'
            if len(stakeholders) > 4:
                stakeholder_text += f" (+{len(stakeholders)-4} more)"

            return f"â€¢ Stakeholders Found: {len(stakeholders)}\nâ€¢ Confidence: {confidence:.1%}\nâ€¢ Stakeholders: {stakeholder_text}"

        elif operation_name == "Response Generation" and isinstance(result, dict):
            has_questions = bool(result.get('questions'))
            suggestions = result.get('suggestions', [])
            content = result.get('content', '')

            content_preview = self._truncate_text(content, 120)

            return f"â€¢ Type: {'Research Questions' if has_questions else 'Guidance Response'}\nâ€¢ Suggestions: {len(suggestions)} generated\nâ€¢ Content Preview: {content_preview}"

        else:
            # Generic formatting for unknown operation types
            if isinstance(result, dict):
                items = []
                for key, value in list(result.items())[:4]:  # Show first 4 items
                    if isinstance(value, (str, int, float, bool)):
                        formatted_value = self._truncate_text(str(value), 60)
                        items.append(f"â€¢ {key}: {formatted_value}")
                    elif isinstance(value, list):
                        items.append(f"â€¢ {key}: {len(value)} items")
                    else:
                        items.append(f"â€¢ {key}: {type(value).__name__}")

                if len(result) > 4:
                    items.append(f"â€¢ ... and {len(result)-4} more fields")

                return "\n".join(items)
            else:
                return self._truncate_text(str(result), 200)

    def _truncate_text(self, text: str, max_length: int) -> str:
        """Truncate text at a logical point if it exceeds max_length."""
        # Handle None values
        if text is None:
            return "Not specified"

        # Ensure text is a string
        text = str(text)

        if len(text) <= max_length:
            return text

        # Try to truncate at word boundary
        truncate_at = text.rfind(' ', 0, max_length - 3)
        if truncate_at == -1:
            truncate_at = max_length - 3

        return text[:truncate_at] + "..."

    def _get_cache_key(self, operation: str, context_hash: str) -> str:
        """Generate cache key for operation."""
        return f"v3_simple:{operation}:{context_hash[:16]}"

    def _get_from_cache(self, cache_key: str) -> Optional[Any]:
        """Get value from cache."""
        if not self.config.enable_caching:
            return None

        # Check request-level cache first
        if cache_key in self.request_cache:
            self.metrics.cache_hits += 1
            return self.request_cache[cache_key]

        # TODO: Add Redis cache integration here
        # For now, only use request-level cache
        self.metrics.cache_misses += 1
        return None

    def _store_in_cache(self, cache_key: str, value: Any):
        """Store value in cache."""
        if not self.config.enable_caching:
            return

        # Store in request-level cache
        self.request_cache[cache_key] = value

        # TODO: Add Redis cache integration here

    async def _execute_with_monitoring(self, operation_name: str, operation_func, *args, **kwargs) -> Any:
        """Execute operation with monitoring and error handling."""

        start_time = time.time()

        try:
            # Add initial thinking step with more descriptive content
            if self.config.enable_thinking_process:
                initial_description = self._get_operation_description(operation_name, "starting")
                self._add_thinking_step(operation_name, "in_progress", initial_description)

            # Execute with timeout
            result = await asyncio.wait_for(
                operation_func(*args, **kwargs),
                timeout=self.config.request_timeout_seconds
            )

            # Calculate duration
            duration_ms = int((time.time() - start_time) * 1000)

            # Update metrics
            setattr(self.metrics, f"{operation_name.lower().replace(' ', '_')}_ms", duration_ms)

            # Update thinking step to completed with detailed results
            if self.config.enable_thinking_process:
                completion_description = self._get_operation_description(operation_name, "completed", result, duration_ms)
                self._add_thinking_step(
                    operation_name,
                    "completed",
                    completion_description,
                    duration_ms
                )

            logger.debug(f"{operation_name} completed in {duration_ms}ms")
            return result

        except asyncio.TimeoutError:
            duration_ms = int((time.time() - start_time) * 1000)
            error_msg = f"{operation_name} timed out after {self.config.request_timeout_seconds}s"

            self.metrics.errors_encountered.append(error_msg)

            if self.config.enable_thinking_process:
                self._add_thinking_step(operation_name, "failed", error_msg, duration_ms)

            logger.warning(error_msg)
            raise

        except Exception as e:
            duration_ms = int((time.time() - start_time) * 1000)
            error_msg = f"{operation_name} failed: {str(e)}"

            self.metrics.errors_encountered.append(error_msg)

            if self.config.enable_thinking_process:
                self._add_thinking_step(operation_name, "failed", error_msg, duration_ms)

            logger.error(error_msg)
            raise

    async def analyze_comprehensive(
        self,
        conversation_context: str,
        latest_input: str,
        messages: List[Dict[str, Any]],
        existing_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Perform comprehensive research analysis using all V3 features with V1/V2 stability.

        This method provides:
        - Sequential processing (no parallel complexity)
        - Smart caching for performance
        - Clear error handling with fallbacks
        - Complete thinking process tracking
        - All V3 enhanced features
        """

        logger.info(f"Starting comprehensive analysis for request {self.request_id}")

        try:
            # Phase 1: Core Analysis (using proven V1/V2 functions)
            context_analysis = await self._execute_with_monitoring(
                "Context Analysis",
                self._analyze_context_enhanced,
                conversation_context, latest_input, existing_context
            )

            intent_analysis = await self._execute_with_monitoring(
                "Intent Analysis",
                self._analyze_intent_enhanced,
                conversation_context, latest_input, messages
            )

            business_validation = await self._execute_with_monitoring(
                "Business Validation",
                self._validate_business_readiness,
                conversation_context, latest_input
            )

            # Phase 2: Enhanced Analysis (V3 features with caching)
            industry_analysis = None
            if self.config.enable_industry_analysis:
                industry_analysis = await self._execute_with_monitoring(
                    "Industry Analysis",
                    self._analyze_industry_enhanced,
                    conversation_context, context_analysis
                )

            stakeholder_analysis = None
            if self.config.enable_stakeholder_detection:
                stakeholder_analysis = await self._execute_with_monitoring(
                    "Stakeholder Detection",
                    self._detect_stakeholders_enhanced,
                    conversation_context, context_analysis, industry_analysis
                )

            conversation_flow = None
            if self.config.enable_conversation_flow:
                conversation_flow = await self._execute_with_monitoring(
                    "Conversation Flow",
                    self._analyze_conversation_flow,
                    context_analysis, intent_analysis, business_validation, messages
                )

            # ðŸŽ¯ SMART HYBRID: Use V1/V2 reliability with V3 enhancements
            # Check if we should generate questions (V3 analysis) but use V1/V2 generation (reliable)
            if self._should_use_enhanced_question_generation(context_analysis, intent_analysis, business_validation, messages):
                logger.info("ðŸŽ¯ SMART HYBRID: Using V1/V2 question generation with V3 enhancements")
                response_content = await self._execute_with_monitoring(
                    "Enhanced Question Generation",
                    self._generate_enhanced_questions_v1v2_hybrid,
                    context_analysis, stakeholder_analysis, intent_analysis, business_validation,
                    industry_analysis, conversation_flow, messages, latest_input, conversation_context
                )
            else:
                # Phase 3: Regular Response Generation (V1/V2 system)
                response_content = await self._execute_with_monitoring(
                    "Response Generation",
                    self._generate_response_enhanced,
                    context_analysis, intent_analysis, business_validation,
                    industry_analysis, stakeholder_analysis, conversation_flow,
                    messages, latest_input, conversation_context
                )

            # Finalize metrics
            self.metrics.end_time = time.time()

            # Build comprehensive response
            # Add V1/V2 compatible metadata fields
            questions_generated = bool(response_content.get("questions"))

            # Determine if we should show confirmation summary
            should_confirm = self._should_present_confirmation_summary(
                context_analysis, intent_analysis, business_validation, messages
            )

            result = {
                "content": response_content["content"],
                "metadata": {
                    # V1/V2 compatible fields
                    "questionCategory": "validation" if questions_generated else "discovery",
                    "researchStage": conversation_flow.get("current_stage", "initial") if conversation_flow else "initial",
                    "suggestions": response_content.get("suggestions", []),
                    "extracted_context": context_analysis,
                    "conversation_stage": "confirming" if should_confirm else "chatting",
                    "show_confirmation": should_confirm and not questions_generated,
                    "questions_generated": questions_generated,
                    "workflow_version": "v3_simple_with_v1_v2_compatibility",
                    "user_intent": intent_analysis,
                    "business_validation": business_validation,
                    "industry_data": industry_analysis,

                    # V3 Simple enhanced fields
                    "stakeholder_analysis": stakeholder_analysis,
                    "conversation_flow": conversation_flow,
                    "confidence_scores": self.metrics.confidence_scores,
                    "request_id": self.request_id
                },
                "questions": response_content.get("questions"),
                "thinking_process": self.thinking_steps if self.config.enable_thinking_process else [],
                "performance_metrics": {
                    "total_duration_ms": self.metrics.total_duration_ms,
                    "success_rate": self.metrics.success_rate,
                    "cache_hits": self.metrics.cache_hits,
                    "cache_misses": self.metrics.cache_misses,
                    "component_timings": {
                        "context_analysis_ms": self.metrics.context_analysis_ms,
                        "intent_analysis_ms": self.metrics.intent_analysis_ms,
                        "business_validation_ms": self.metrics.business_validation_ms,
                        "industry_analysis_ms": self.metrics.industry_analysis_ms,
                        "stakeholder_detection_ms": self.metrics.stakeholder_detection_ms,
                        "conversation_flow_ms": self.metrics.conversation_flow_ms,
                        "response_generation_ms": self.metrics.response_generation_ms
                    }
                }
            }

            logger.info(f"Comprehensive analysis completed in {self.metrics.total_duration_ms}ms")
            return result

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"Comprehensive analysis failed: {e}")
            logger.error(f"Full error traceback: {error_details}")

            # Add final error thinking step with detailed information
            if self.config.enable_thinking_process:
                self._add_thinking_step(
                    "Analysis Failed",
                    "failed",
                    f"V3 Simple analysis failed with error: {str(e)}\n\nError details: {error_details[:500]}..."
                )

            # Use V1 fallback if enabled
            if self.config.enable_v1_fallback:
                logger.info("Falling back to V1 analysis")
                self.metrics.fallback_used = True
                return await self._fallback_to_v1_analysis(conversation_context, latest_input, messages, existing_context)
            else:
                # Provide a more user-friendly error response instead of crashing
                logger.error("V1 fallback disabled, providing error response")
                return {
                    "content": "I apologize, but I'm experiencing technical difficulties analyzing your request. Please try again or contact support if the issue persists.",
                    "metadata": {
                        "questionCategory": "discovery",
                        "researchStage": "initial",
                        "suggestions": ["Try rephrasing your request", "Contact support if issues persist"],
                        "extracted_context": {},
                        "conversation_stage": "error",
                        "show_confirmation": False,
                        "questions_generated": False,
                        "workflow_version": "v3_simple_error_fallback",
                        "error": str(e),
                        "request_id": self.request_id
                    },
                    "questions": None,
                    "thinking_process": self.thinking_steps if self.config.enable_thinking_process else [],
                    "performance_metrics": {
                        "total_duration_ms": self.metrics.total_duration_ms,
                        "success_rate": 0.0,
                        "errors_encountered": self.metrics.errors_encountered + [str(e)]
                    }
                }

    def _should_use_enhanced_question_generation(self, context_analysis: Dict[str, Any], intent_analysis: Dict[str, Any],
                                               business_validation: Dict[str, Any], messages: List[Dict[str, Any]]) -> bool:
        """Determine if we should use enhanced question generation (V1/V2 reliability + V3 features)."""

        try:
            # Check if user explicitly requested questions
            user_intent = intent_analysis.get('intent', '')
            user_confirmed_explicitly = user_intent == 'question_request'

            # Check if business context is ready
            v3_ready = business_validation.get('ready_for_questions', False)

            # Check conversation depth (minimum 2 exchanges)
            conversation_depth = len([msg for msg in messages if msg.get('role') == 'user'])
            min_conversation_depth = 2

            # Check context clarity
            business_clarity = business_validation.get('business_clarity', {})
            idea_clarity = business_clarity.get('idea_clarity', 0.0)
            customer_clarity = business_clarity.get('customer_clarity', 0.0)
            problem_clarity = business_clarity.get('problem_clarity', 0.0)

            context_sufficiently_clear = (
                idea_clarity >= 0.8 and
                customer_clarity >= 0.8 and
                problem_clarity >= 0.8
            )

            # Enhanced question generation conditions
            should_use_enhanced = (
                v3_ready and  # V3 system says ready
                conversation_depth >= min_conversation_depth and  # Minimum conversation depth
                user_confirmed_explicitly and  # User explicitly confirmed
                context_sufficiently_clear  # Context is clear enough
            )

            logger.info(f"Enhanced question generation decision: should_use_enhanced={should_use_enhanced}")
            logger.info(f"  - V3 ready: {v3_ready}")
            logger.info(f"  - Conversation depth: {conversation_depth} (min: {min_conversation_depth})")
            logger.info(f"  - User confirmed explicitly: {user_confirmed_explicitly}")
            logger.info(f"  - Context clarity: idea={idea_clarity:.2f}, customer={customer_clarity:.2f}, problem={problem_clarity:.2f}")
            logger.info(f"  - Context sufficiently clear: {context_sufficiently_clear}")

            return should_use_enhanced

        except Exception as e:
            logger.warning(f"Error in enhanced question generation decision: {e}")
            return False

    def _should_use_emergency_bypass(self, context_analysis: Dict[str, Any], intent_analysis: Dict[str, Any],
                                   business_validation: Dict[str, Any], messages: List[Dict[str, Any]]) -> bool:
        """Determine if we should use emergency bypass to generate questions immediately."""

        try:
            # Use the same logic as enhanced question generation
            return self._should_use_enhanced_question_generation(context_analysis, intent_analysis, business_validation, messages)

        except Exception as e:
            logger.warning(f"Error in emergency bypass decision: {e}")
            return False

    def _create_emergency_questions_response(self, context_analysis: Dict[str, Any], stakeholder_analysis: Optional[Dict[str, Any]],
                                           intent_analysis: Dict[str, Any], business_validation: Dict[str, Any],
                                           industry_analysis: Optional[Dict[str, Any]], conversation_flow: Dict[str, Any]) -> Dict[str, Any]:
        """Create emergency questions response to avoid hanging V1/V2 calls."""

        try:
            logger.info("ðŸš¨ EMERGENCY BYPASS: Creating questions immediately to avoid hanging V1/V2 calls")

            # Extract context for questions
            business_idea = context_analysis.get('business_idea') or context_analysis.get('businessIdea', 'your business')
            target_customer = context_analysis.get('target_customer') or context_analysis.get('targetCustomer', 'customers')
            problem = context_analysis.get('problem', 'challenges they face')

            # Create immediate working questions
            emergency_questions = {
                "primaryStakeholders": [
                    {
                        "name": f"{target_customer.title()}",
                        "description": f"The primary users of the {business_idea}",
                        "questions": {
                            "problemDiscovery": [
                                f"What challenges do you currently face with {business_idea.split()[-1] if business_idea else 'this service'}?",
                                f"How do you currently handle {problem.split('.')[0] if problem else 'these needs'}?",
                                "What's the most frustrating part of your current situation?",
                                "How often do you encounter these problems?",
                                "What would make this easier for you?"
                            ],
                            "solutionValidation": [
                                f"Would a {business_idea} help solve your problem?",
                                "What features would be most important to you?",
                                "How much would you be willing to pay for this service?",
                                "What would convince you to try this service?",
                                "What concerns would you have about using this?"
                            ],
                            "followUp": [
                                "Would you recommend this to others in your situation?",
                                "What else should we know about your needs?",
                                "Any other feedback or suggestions?"
                            ]
                        }
                    }
                ],
                "secondaryStakeholders": [
                    {
                        "name": "Family Members",
                        "description": f"People who care about or help {target_customer}",
                        "questions": {
                            "problemDiscovery": [
                                f"How do you currently help {target_customer} with their needs?",
                                "What challenges do you see them facing?",
                                "How does this affect you or your family?"
                            ],
                            "solutionValidation": [
                                f"Would you support using a {business_idea}?",
                                "What would you want to see in this service?",
                                "What concerns would you have?"
                            ],
                            "followUp": [
                                "Would you help them access this service?",
                                "Any other thoughts?"
                            ]
                        }
                    }
                ],
                "timeEstimate": {
                    "totalQuestions": 18,
                    "estimatedMinutes": "36-54",
                    "breakdown": {"primary": 13, "secondary": 5}
                }
            }

            # Return immediate response with questions
            return {
                "content": f"Perfect! I've generated comprehensive research questions for your {business_idea}. These questions will help you validate the market need and refine your solution.",
                "questions": emergency_questions,
                "suggestions": [],
                "metadata": {
                    "suggestions": [],
                    "contextual_suggestions": [],
                    "extracted_context": context_analysis,
                    "user_intent": intent_analysis,
                    "business_readiness": business_validation,
                    "industry_analysis": industry_analysis,
                    "stakeholder_analysis": stakeholder_analysis,
                    "conversation_flow": conversation_flow,
                    "request_id": self.request_id,
                    "emergency_bypass": True,
                    "questionCategory": "comprehensive",
                    "researchStage": "questions_generated",
                    "conversation_stage": "questions_ready",
                    "show_confirmation": False,
                    "questions_generated": True,
                    "workflow_version": "v3_simple_emergency_bypass"
                }
            }

        except Exception as e:
            logger.error(f"Error in emergency questions response: {e}")
            # Ultimate fallback
            return {
                "content": "I've generated basic research questions for your business idea.",
                "questions": {
                    "problemDiscovery": [
                        "What challenges do you currently face?",
                        "How do you handle this now?",
                        "What would make this easier?"
                    ],
                    "solutionValidation": [
                        "Would this solution help you?",
                        "What features are most important?",
                        "How much would you pay for this?"
                    ],
                    "followUp": [
                        "Any other thoughts?",
                        "Would you recommend this to others?"
                    ]
                },
                "suggestions": [],
                "metadata": {
                    "emergency_fallback": True,
                    "request_id": self.request_id
                }
            }

    async def _generate_enhanced_questions_v1v2_hybrid(self, context_analysis: Dict[str, Any], stakeholder_analysis: Optional[Dict[str, Any]],
                                                     intent_analysis: Dict[str, Any], business_validation: Dict[str, Any],
                                                     industry_analysis: Optional[Dict[str, Any]], conversation_flow: Optional[Dict[str, Any]],
                                                     messages: List[Dict[str, Any]], latest_input: str, conversation_context: str) -> Dict[str, Any]:
        """Generate enhanced questions using V1/V2 reliability with V3 stakeholder enhancements."""

        try:
            logger.info("ðŸŽ¯ Starting V3 direct question generation (no V1/V2 imports)")

            # Generate enhanced questions directly using V3 stakeholder data (no V1/V2 imports)
            if stakeholder_analysis:
                logger.info("ðŸŽ¯ Creating enhanced questions using V3 stakeholder data")
                enhanced_questions = self._enhance_questions_with_stakeholders({}, stakeholder_analysis, context_analysis)
            else:
                logger.info("ðŸ“ Creating basic questions (no stakeholder data)")
                enhanced_questions = self._create_simple_fallback_questions(context_analysis, None)["questions"]

            # Convert comprehensive questions to simple format to avoid validation issues
            simple_questions = self._convert_to_simple_format(enhanced_questions)

            # Return enhanced response with simple format
            return {
                "content": f"Perfect! I've generated comprehensive research questions for your business idea. These questions will help you validate the market need and refine your solution.",
                "questions": simple_questions,
                "suggestions": [],
                "metadata": {
                    "suggestions": [],
                    "contextual_suggestions": [],
                    "extracted_context": context_analysis,
                    "user_intent": intent_analysis,
                    "business_readiness": business_validation,
                    "industry_analysis": industry_analysis,
                    "stakeholder_analysis": stakeholder_analysis,
                    "conversation_flow": conversation_flow,
                    "request_id": self.request_id,
                    "v1v2_hybrid": True,
                    "questionCategory": "comprehensive",
                    "researchStage": "questions_generated",
                    "conversation_stage": "questions_ready",
                    "show_confirmation": False,
                    "questions_generated": True,
                    "workflow_version": "v3_simple_v1v2_hybrid",
                    "comprehensive_questions": enhanced_questions  # Store comprehensive format in metadata
                }
            }

        except Exception as e:
            logger.error(f"Error in V1/V2 hybrid question generation: {e}")
            # Fallback to simple questions
            return self._create_simple_fallback_questions(context_analysis, stakeholder_analysis)

    def _convert_to_simple_format(self, comprehensive_questions: Dict[str, Any]) -> Dict[str, Any]:
        """Convert comprehensive questions format to simple format expected by validation."""

        try:
            # Extract all questions from comprehensive format
            problem_discovery = []
            solution_validation = []
            follow_up = []

            # Extract from primary stakeholders
            for stakeholder in comprehensive_questions.get("primaryStakeholders", []):
                if isinstance(stakeholder, dict) and "questions" in stakeholder:
                    questions = stakeholder["questions"]
                    problem_discovery.extend(questions.get("problemDiscovery", []))
                    solution_validation.extend(questions.get("solutionValidation", []))
                    follow_up.extend(questions.get("followUp", []))

            # Extract from secondary stakeholders
            for stakeholder in comprehensive_questions.get("secondaryStakeholders", []):
                if isinstance(stakeholder, dict) and "questions" in stakeholder:
                    questions = stakeholder["questions"]
                    problem_discovery.extend(questions.get("problemDiscovery", []))
                    solution_validation.extend(questions.get("solutionValidation", []))
                    follow_up.extend(questions.get("followUp", []))

            # Return simple format that matches validation expectations
            return {
                "problemDiscovery": problem_discovery[:10],  # Limit to reasonable number
                "solutionValidation": solution_validation[:10],
                "followUp": follow_up[:5]
            }

        except Exception as e:
            logger.warning(f"Error converting to simple format: {e}")
            # Return basic fallback
            return {
                "problemDiscovery": [
                    "What challenges do you currently face?",
                    "How do you handle this now?",
                    "What would make this easier?"
                ],
                "solutionValidation": [
                    "Would this solution help you?",
                    "What features are most important?",
                    "How much would you pay for this?"
                ],
                "followUp": [
                    "Any other thoughts?",
                    "Would you recommend this to others?"
                ]
            }

    def _enhance_questions_with_stakeholders(self, v1v2_questions: Dict[str, Any], stakeholder_analysis: Dict[str, Any],
                                           context_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Enhance V1/V2 questions with V3 stakeholder data."""

        try:
            # If V1/V2 already has good structure, use it
            if isinstance(v1v2_questions, dict) and 'primaryStakeholders' in v1v2_questions:
                logger.info("âœ… V1/V2 questions already have stakeholder structure")
                return v1v2_questions

            # Extract context for enhancement
            business_idea = context_analysis.get('business_idea') or context_analysis.get('businessIdea', 'your business')
            target_customer = context_analysis.get('target_customer') or context_analysis.get('targetCustomer', 'customers')
            problem = context_analysis.get('problem', 'challenges they face')

            # Get stakeholder data
            primary_stakeholders = stakeholder_analysis.get('primary', [])
            secondary_stakeholders = stakeholder_analysis.get('secondary', [])

            # Create enhanced structure using V3 stakeholders but V1/V2 question content
            enhanced_questions = {
                "primaryStakeholders": [],
                "secondaryStakeholders": [],
                "timeEstimate": {
                    "totalQuestions": 0,
                    "estimatedMinutes": "0-0",
                    "breakdown": {"primary": 0, "secondary": 0}
                }
            }

            # Add primary stakeholders with enhanced questions
            for stakeholder in primary_stakeholders:
                stakeholder_name = stakeholder.get('name', target_customer.title()) if isinstance(stakeholder, dict) else str(stakeholder)
                stakeholder_desc = stakeholder.get('description', f'Users of the {business_idea}') if isinstance(stakeholder, dict) else f'Users of the {business_idea}'

                enhanced_questions["primaryStakeholders"].append({
                    "name": stakeholder_name,
                    "description": stakeholder_desc,
                    "questions": {
                        "problemDiscovery": [
                            f"What challenges do you currently face with {business_idea.split()[-1] if business_idea else 'this service'}?",
                            f"How do you currently handle {problem.split('.')[0] if problem else 'these needs'}?",
                            "What's the most frustrating part of your current situation?",
                            "How often do you encounter these problems?",
                            "What would make this easier for you?"
                        ],
                        "solutionValidation": [
                            f"Would a {business_idea} help solve your problem?",
                            "What features would be most important to you?",
                            "How much would you be willing to pay for this service?",
                            "What would convince you to try this service?",
                            "What concerns would you have about using this?"
                        ],
                        "followUp": [
                            "Would you recommend this to others in your situation?",
                            "What else should we know about your needs?",
                            "Any other feedback or suggestions?"
                        ]
                    }
                })

            # Add secondary stakeholders
            for stakeholder in secondary_stakeholders:
                stakeholder_name = stakeholder.get('name', 'Family Members') if isinstance(stakeholder, dict) else str(stakeholder)
                stakeholder_desc = stakeholder.get('description', f'People who influence {target_customer}') if isinstance(stakeholder, dict) else f'People who influence {target_customer}'

                enhanced_questions["secondaryStakeholders"].append({
                    "name": stakeholder_name,
                    "description": stakeholder_desc,
                    "questions": {
                        "problemDiscovery": [
                            f"How do you currently help {target_customer} with their needs?",
                            "What challenges do you see them facing?",
                            "How does this affect you or your family?"
                        ],
                        "solutionValidation": [
                            f"Would you support using a {business_idea}?",
                            "What would you want to see in this service?",
                            "What concerns would you have?"
                        ],
                        "followUp": [
                            "Would you help them access this service?",
                            "Any other thoughts?"
                        ]
                    }
                })

            # Calculate time estimates
            primary_questions = sum(len(s["questions"]["problemDiscovery"]) + len(s["questions"]["solutionValidation"]) + len(s["questions"]["followUp"])
                                  for s in enhanced_questions["primaryStakeholders"])
            secondary_questions = sum(len(s["questions"]["problemDiscovery"]) + len(s["questions"]["solutionValidation"]) + len(s["questions"]["followUp"])
                                    for s in enhanced_questions["secondaryStakeholders"])
            total_questions = primary_questions + secondary_questions

            # Calculate realistic interview time (2.5 minutes per question + 20% buffer)
            base_time = int(total_questions * 2.5)
            buffer_time = int(base_time * 0.2)
            min_time = base_time
            max_time = base_time + buffer_time

            enhanced_questions["timeEstimate"] = {
                "totalQuestions": total_questions,
                "estimatedMinutes": f"{min_time}-{max_time}",
                "breakdown": {
                    "primary": primary_questions,
                    "secondary": secondary_questions,
                    "baseTime": min_time,
                    "withBuffer": max_time,
                    "perQuestion": 2.5
                }
            }

            logger.info(f"âœ… Enhanced questions with V3 stakeholders: {len(enhanced_questions['primaryStakeholders'])} primary, {len(enhanced_questions['secondaryStakeholders'])} secondary")
            return enhanced_questions

        except Exception as e:
            logger.warning(f"Error enhancing questions with stakeholders: {e}")
            return v1v2_questions  # Return original V1/V2 questions

    def _create_simple_fallback_questions(self, context_analysis: Dict[str, Any], stakeholder_analysis: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Create simple fallback questions when everything else fails."""

        business_idea = context_analysis.get('business_idea') or context_analysis.get('businessIdea', 'your business')
        target_customer = context_analysis.get('target_customer') or context_analysis.get('targetCustomer', 'customers')

        return {
            "content": f"I've generated basic research questions for your {business_idea}.",
            "questions": {
                "primaryStakeholders": [
                    {
                        "name": target_customer.title() if target_customer else "Target Customers",
                        "description": f"The primary users of your {business_idea}",
                        "questions": {
                            "problemDiscovery": [
                                "What challenges do you face with this need?",
                                "How do you currently handle this?",
                                "What would make this easier for you?"
                            ],
                            "solutionValidation": [
                                "Would this solution help you?",
                                "What features would be most important?",
                                "How much would you pay for this?"
                            ],
                            "followUp": [
                                "Any other thoughts?",
                                "Would you recommend this to others?"
                            ]
                        }
                    }
                ],
                "secondaryStakeholders": [],
                "timeEstimate": {
                    "totalQuestions": 8,
                    "estimatedMinutes": "16-24",
                    "breakdown": {"primary": 8, "secondary": 0}
                }
            },
            "suggestions": [],
            "metadata": {
                "v1v2_hybrid_fallback": True,
                "request_id": self.request_id
            }
        }

    async def _analyze_context_enhanced(self, conversation_context: str, latest_input: str, existing_context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Enhanced context analysis using proven V1/V2 patterns with caching."""

        # Validate inputs to prevent NoneType errors
        if conversation_context is None:
            conversation_context = ""
            logger.warning("conversation_context was None, using empty string")

        if latest_input is None:
            latest_input = ""
            logger.warning("latest_input was None, using empty string")

        # Ensure inputs are strings
        conversation_context = str(conversation_context)
        latest_input = str(latest_input)

        # Generate cache key
        import hashlib
        context_hash = hashlib.md5(f"{conversation_context}{latest_input}".encode()).hexdigest()
        cache_key = self._get_cache_key("context", context_hash)

        # Check cache
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            return cached_result

        # Import proven V1/V2 function
        from backend.api.routes.customer_research import extract_context_with_llm

        # Create context object for V1/V2 compatibility
        context_obj = type('Context', (), {
            'businessIdea': existing_context.get('businessIdea') if existing_context else None,
            'targetCustomer': existing_context.get('targetCustomer') if existing_context else None,
            'problem': existing_context.get('problem') if existing_context else None,
            'stage': existing_context.get('stage') if existing_context else None
        })()

        # Use proven V1/V2 function with LLM interaction capture
        llm_client = self._get_llm_client()

        # Capture the prompt that will be sent to LLM (reconstruct from function)
        context_prompt = f"""Analyze this customer research conversation and extract the business context.

Conversation:
{conversation_context}

Latest user input: "{latest_input}"

Extract the following information and return ONLY valid JSON:

{{
  "business_idea": "what they want to build/create",
  "target_customer": "who will use this",
  "problem": "what problem this solves"
}}

Rules:
- Extract ANY information mentioned, even if partial or incomplete
- Use the user's exact words when possible
- Be generous in extraction - capture hints and implications
- Only use null if absolutely no information is available

Return only valid JSON:"""

        result = await extract_context_with_llm(
            llm_service=llm_client,
            conversation_context=conversation_context,
            latest_input=latest_input
        )

        # Capture the LLM interaction for thinking process with raw response
        # Try to get the raw LLM response from logs if available
        raw_response = str(result)
        try:
            import json
            if isinstance(result, dict):
                raw_response = json.dumps(result, indent=2, ensure_ascii=False)
        except:
            pass

        self._capture_llm_interaction(
            "Context Analysis",
            context_prompt,
            raw_response,
            {"conversation_length": len(conversation_context), "input_length": len(latest_input)}
        )

        # Store in cache
        self._store_in_cache(cache_key, result)

        # Update confidence score
        self.metrics.confidence_scores["context_analysis"] = result.get("confidence", 0.8)

        return result

    async def _analyze_intent_enhanced(self, conversation_context: str, latest_input: str, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Enhanced intent analysis using proven V1/V2 patterns with caching."""

        # Validate inputs to prevent NoneType errors
        if conversation_context is None:
            conversation_context = ""
            logger.warning("conversation_context was None in intent analysis, using empty string")

        if latest_input is None:
            latest_input = ""
            logger.warning("latest_input was None in intent analysis, using empty string")

        if messages is None:
            messages = []
            logger.warning("messages was None in intent analysis, using empty list")

        # Ensure inputs are strings
        conversation_context = str(conversation_context)
        latest_input = str(latest_input)

        # Generate cache key
        import hashlib
        intent_hash = hashlib.md5(f"{conversation_context}{latest_input}".encode()).hexdigest()
        cache_key = self._get_cache_key("intent", intent_hash)

        # Check cache
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            return cached_result

        # Import proven V1/V2 function
        from backend.api.routes.customer_research import analyze_user_intent_with_llm

        # Convert dict messages to Message objects for V1/V2 compatibility
        from backend.api.routes.customer_research import Message as V1Message
        v1_messages = []
        for msg in messages:
            if isinstance(msg, dict):
                v1_messages.append(V1Message(
                    id=msg.get('id', f'msg_{int(time.time())}'),
                    content=msg.get('content', ''),
                    role=msg.get('role', 'user'),
                    timestamp=msg.get('timestamp', datetime.now().isoformat()),
                    metadata=msg.get('metadata')
                ))
            else:
                v1_messages.append(msg)

        # Use proven V1/V2 function with LLM interaction capture
        llm_client = self._get_llm_client()

        # Get the last assistant message for context
        last_assistant_message = ""
        for msg in reversed(v1_messages):
            if msg.role == "assistant":
                last_assistant_message = msg.content
                break

        # Capture the prompt that will be sent to LLM
        intent_prompt = f"""Analyze the user's latest response to determine their intent in this customer research conversation.

Last assistant message: "{last_assistant_message}"
User's response: "{latest_input}"

Full conversation context:
{conversation_context}

Determine the user's intent and return ONLY valid JSON:

{{
  "intent": "confirmation|rejection|clarification|continuation|question_request",
  "confidence": 0.85,
  "reasoning": "Brief explanation of why you classified it this way",
  "specific_feedback": "What specifically the user is confirming, rejecting, or clarifying",
  "next_action": "What the assistant should do next"
}}

Intent definitions:
- "confirmation": User is agreeing/confirming the assistant's understanding
- "rejection": User is disagreeing/rejecting the assistant's understanding
- "clarification": User wants to add more info or correct something
- "continuation": User is providing more information to continue the conversation
- "question_request": User explicitly wants research questions generated"""

        result = await analyze_user_intent_with_llm(
            llm_service=llm_client,
            conversation_context=conversation_context,
            latest_input=latest_input,
            messages=v1_messages
        )

        # Capture the LLM interaction for thinking process with formatted response
        raw_response = str(result)
        try:
            import json
            if isinstance(result, dict):
                raw_response = json.dumps(result, indent=2, ensure_ascii=False)
        except:
            pass

        self._capture_llm_interaction(
            "Intent Analysis",
            intent_prompt,
            raw_response,
            {"last_assistant_message": last_assistant_message, "message_count": len(v1_messages)}
        )

        # Store in cache
        self._store_in_cache(cache_key, result)

        # Update confidence score
        self.metrics.confidence_scores["intent_analysis"] = result.get("confidence", 0.8)

        return result

    async def _validate_business_readiness(self, conversation_context: str, latest_input: str) -> Dict[str, Any]:
        """Enhanced business readiness validation using proven V1/V2 patterns."""

        # Validate inputs to prevent NoneType errors
        if conversation_context is None:
            conversation_context = ""
            logger.warning("conversation_context was None in business validation, using empty string")

        if latest_input is None:
            latest_input = ""
            logger.warning("latest_input was None in business validation, using empty string")

        # Ensure inputs are strings
        conversation_context = str(conversation_context)
        latest_input = str(latest_input)

        # Import proven V1/V2 function
        from backend.api.routes.customer_research import validate_business_readiness_with_llm

        # Use proven V1/V2 function with LLM interaction capture
        llm_client = self._get_llm_client()

        # Capture the prompt that will be sent to LLM
        validation_prompt = f"""Analyze this customer research conversation to determine if enough information has been gathered to show a CONFIRMATION SUMMARY before generating research questions.

Conversation:
{conversation_context}

Latest user input: "{latest_input}"

Evaluate the conversation and return ONLY valid JSON:

{{
  "ready_for_questions": true,
  "confidence": 0.85,
  "reasoning": "Detailed explanation of readiness assessment",
  "missing_elements": ["element1", "element2"],
  "conversation_quality": "high",
  "business_clarity": {{
    "idea_clarity": 0.9,
    "customer_clarity": 0.8,
    "problem_clarity": 0.7
  }},
  "recommendations": ["suggestion1", "suggestion2"]
}}

Assessment criteria for CONFIRMATION readiness:
1. Business idea clarity - Is there a clear, specific understanding of what they're building?
2. Target customer definition - Are the target users/customers clearly identified?
3. Problem articulation - Is the specific problem they're solving clearly explained?
4. Content completeness - Is there enough detail to create meaningful research questions?
5. Context depth - Do we understand the business context, not just surface-level information?

Be conservative but focus on content completeness, not message count."""

        result = await validate_business_readiness_with_llm(
            llm_service=llm_client,
            conversation_context=conversation_context,
            latest_input=latest_input
        )

        # Capture the LLM interaction for thinking process with formatted response
        raw_response = str(result)
        try:
            import json
            if isinstance(result, dict):
                raw_response = json.dumps(result, indent=2, ensure_ascii=False)
        except:
            pass

        self._capture_llm_interaction(
            "Business Validation",
            validation_prompt,
            raw_response,
            {"conversation_length": len(conversation_context)}
        )

        # Update confidence score
        self.metrics.confidence_scores["business_validation"] = result.get("confidence", 0.8)

        return result

    async def _analyze_industry_enhanced(self, conversation_context: str, context_analysis: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Enhanced industry analysis with caching."""

        # Generate cache key
        import hashlib
        industry_hash = hashlib.md5(f"{conversation_context}{str(context_analysis)}".encode()).hexdigest()
        cache_key = self._get_cache_key("industry", industry_hash)

        # Check cache
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            return cached_result

        try:
            # Import proven V1/V2 function
            from backend.api.routes.customer_research import classify_industry_with_llm

            # Use proven V1/V2 function with LLM interaction capture
            llm_client = self._get_llm_client()

            # Capture the prompt for industry classification
            industry_prompt = f"""Analyze this customer research conversation and classify the industry/domain.

Conversation:
{conversation_context}

Based on the business idea, target customers, and problem described, identify the most relevant industry classification and return ONLY valid JSON:

{{
  "industry": "industry_name",
  "confidence": 0.85,
  "reasoning": "Brief explanation of why this industry was identified",
  "sub_categories": ["specific", "domain", "areas"]
}}

Industry options (choose the most specific):
- "ux_research" - UX research, design research, user experience
- "product_management" - Product development, feature management, roadmaps
- "saas" - Software as a Service, B2B platforms, business tools
- "healthcare" - Medical, patient care, clinical systems
- "education" - Learning, teaching, academic systems
- "ecommerce" - Online retail, marketplaces, selling platforms
- "fintech" - Financial services, banking, payments
- "hr_tech" - Human resources, recruiting, employee management
- "marketing_tech" - Marketing automation, analytics, campaigns
- "data_analytics" - Business intelligence, data processing, insights
- "general" - General business tools or unclear domain"""

            result = await classify_industry_with_llm(
                llm_service=llm_client,
                conversation_context=conversation_context,
                latest_input=""  # Not needed for industry classification
            )

            # Capture the LLM interaction for thinking process
            raw_response = str(result)
            try:
                import json
                if isinstance(result, dict):
                    raw_response = json.dumps(result, indent=2, ensure_ascii=False)
            except:
                pass

            self._capture_llm_interaction(
                "Industry Analysis",
                industry_prompt,
                raw_response,
                {"context_analysis": str(context_analysis)[:100]}
            )

            # Store in cache
            self._store_in_cache(cache_key, result)

            # Update confidence score
            self.metrics.confidence_scores["industry_analysis"] = result.get("confidence", 0.7)

            return result

        except Exception as e:
            logger.warning(f"Industry analysis failed: {e}")
            return None

    async def _detect_stakeholders_enhanced(self, conversation_context: str, context_analysis: Dict[str, Any], industry_analysis: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Enhanced stakeholder detection with caching."""

        # Generate cache key
        import hashlib
        stakeholder_hash = hashlib.md5(f"{conversation_context}{str(context_analysis)}{str(industry_analysis)}".encode()).hexdigest()
        cache_key = self._get_cache_key("stakeholders", stakeholder_hash)

        # Check cache
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            return cached_result

        try:
            # Use Instructor like the working pattern processor
            logger.info("ðŸš€ Using Instructor for stakeholder detection (following pattern processor approach)")

            # Extract business context
            business_idea = context_analysis.get('business_idea') or context_analysis.get('businessIdea', '')
            target_customer = context_analysis.get('target_customer') or context_analysis.get('targetCustomer', '')
            problem = context_analysis.get('problem', '')

            # Create optimized prompt for stakeholder detection
            prompt = f"""Analyze this business context and identify key stakeholders for customer research:

BUSINESS CONTEXT:
- Business Idea: {business_idea}
- Target Customer: {target_customer}
- Problem: {problem}

CONVERSATION CONTEXT:
{conversation_context}

TASK: Identify stakeholders who should be interviewed for customer research.

Consider these questions:
- Who are the primary decision makers and end users?
- Who influences buying decisions?
- Who is directly affected by the problem?
- Who might resist or support the solution?
- Who has budget authority or technical influence?

REQUIREMENTS:
- Primary stakeholders: 1-3 most important groups (decision makers, primary users)
- Secondary stakeholders: 0-3 supporting groups (influencers, supporters)
- Each stakeholder needs a clear name and specific description of their role
- Industry classification should be specific and relevant

Make descriptions specific to this exact business situation, not generic."""

            # Use Instructor like the pattern processor does
            from backend.models.comprehensive_questions import StakeholderDetection
            from backend.services.llm.instructor_gemini_client import InstructorGeminiClient

            instructor_client = InstructorGeminiClient()

            # Generate structured output using Instructor (following pattern processor approach)
            stakeholder_data = await instructor_client.generate_with_model_async(
                prompt=prompt,
                model_class=StakeholderDetection,
                temperature=0.0,  # Use deterministic output like pattern processor
                system_instruction="You are an expert business analyst. Identify the most relevant stakeholders for customer research interviews.",
                response_mime_type="application/json",  # Force JSON output like pattern processor
                max_output_tokens=8000  # Reasonable token limit
            )

            logger.info(f"âœ… Instructor detected stakeholders successfully")
            logger.info(f"Primary: {[s['name'] for s in stakeholder_data.primary]}")
            logger.info(f"Secondary: {[s['name'] for s in stakeholder_data.secondary]}")
            logger.info(f"Industry: {stakeholder_data.industry}")

            # Convert to dict for API compatibility
            result = stakeholder_data.dict()

            # Store in cache
            self._store_in_cache(cache_key, result)

            # Update confidence score
            self.metrics.confidence_scores["stakeholder_analysis"] = 0.9  # High confidence for Instructor

            return result

        except Exception as e:
            logger.warning(f"Instructor stakeholder detection failed: {e}")
            # Fallback to simple stakeholder detection
            logger.info("Using simple fallback stakeholder detection")

            business_idea = context_analysis.get('business_idea') or context_analysis.get('businessIdea', '')
            target_customer = context_analysis.get('target_customer') or context_analysis.get('targetCustomer', '')

            # Create simple fallback stakeholders
            fallback_result = {
                "primary": [
                    {
                        "name": target_customer.title() if target_customer else "Primary Users",
                        "description": f"The main users of the {business_idea}" if business_idea else "Primary users of the service"
                    }
                ],
                "secondary": [
                    {
                        "name": "Family Members",
                        "description": f"People who support or influence {target_customer}" if target_customer else "Supporting family members"
                    }
                ],
                "industry": "general"
            }

            logger.info(f"âœ… Using fallback stakeholders: {fallback_result}")
            return fallback_result

    async def _analyze_conversation_flow(self, context_analysis: Dict[str, Any], intent_analysis: Dict[str, Any],
                                       business_validation: Dict[str, Any], messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze conversation flow and determine next steps."""

        try:
            # Import proven V1/V2 function
            from backend.api.routes.customer_research import determine_research_stage_from_context

            # Use proven V1/V2 function
            stage = determine_research_stage_from_context(context_analysis)

            # Enhanced flow analysis
            flow_result = {
                "current_stage": stage,
                "next_recommended_action": self._determine_next_action(context_analysis, intent_analysis, business_validation),
                "conversation_quality": self._assess_conversation_quality(messages),
                "readiness_for_questions": business_validation.get("ready_for_questions", False)
            }

            # Update confidence score
            self.metrics.confidence_scores["conversation_flow"] = 0.9  # High confidence for rule-based analysis

            return flow_result

        except Exception as e:
            logger.warning(f"Conversation flow analysis failed: {e}")
            return {"current_stage": "initial", "next_recommended_action": "gather_business_idea"}

    def _determine_next_action(self, context: Dict[str, Any], intent: Dict[str, Any], validation: Dict[str, Any]) -> str:
        """Determine the next recommended action based on analysis."""

        if not context.get("businessIdea"):
            return "gather_business_idea"
        elif not context.get("targetCustomer"):
            return "gather_target_customer"
        elif not context.get("problem"):
            return "gather_problem_statement"
        elif not validation.get("ready_for_questions", False):
            return "clarify_business_context"
        else:
            return "generate_research_questions"

    def _assess_conversation_quality(self, messages: List[Dict[str, Any]]) -> float:
        """Assess the quality of the conversation."""

        if len(messages) < 2:
            return 0.3
        elif len(messages) < 5:
            return 0.6
        else:
            return 0.9

    def _should_present_confirmation_summary(self, context_analysis: Dict[str, Any],
                                           intent_analysis: Dict[str, Any],
                                           business_validation: Dict[str, Any],
                                           messages: List[Dict[str, Any]]) -> bool:
        """Determine if we should present a confirmation summary."""

        # Simplified logic - only present confirmation if we have all three key elements
        has_business_idea = bool(context_analysis.get("business_idea"))
        has_target_customer = bool(context_analysis.get("target_customer"))
        has_problem = bool(context_analysis.get("problem"))

        # Check if user hasn't already confirmed
        user_intent = intent_analysis.get("intent", "")
        has_confirmed = user_intent in ["confirmation", "question_request"]

        # Present confirmation only if we have all three elements and user hasn't confirmed
        return (
            has_business_idea and
            has_target_customer and
            has_problem and
            not has_confirmed and
            len(messages) >= 3  # At least 3 exchanges to gather all info
        )

    def _generate_confirmation_summary(self, context_analysis: Dict[str, Any],
                                     intent_analysis: Dict[str, Any]) -> str:
        """Generate a confirmation summary for the user to validate."""

        business_idea = context_analysis.get("business_idea", "your business idea")
        target_customer = context_analysis.get("target_customer", "")
        problem = context_analysis.get("problem", "")

        summary_parts = [
            f"**Business Idea:** {business_idea}"
        ]

        if target_customer:
            summary_parts.append(f"**Target Customer:** {target_customer}")

        if problem:
            summary_parts.append(f"**Problem Being Solved:** {problem}")

        summary = "\n".join(summary_parts)

        return f"""Perfect! Let me summarize what I understand about your business:

{summary}

Is this summary accurate? If yes, I'll generate targeted research questions to help you validate this business idea with real customers."""

    async def _generate_response_enhanced(self, context_analysis: Dict[str, Any], intent_analysis: Dict[str, Any],
                                        business_validation: Dict[str, Any], industry_analysis: Optional[Dict[str, Any]],
                                        stakeholder_analysis: Optional[Dict[str, Any]], conversation_flow: Dict[str, Any],
                                        messages: List[Dict[str, Any]], latest_input: str, conversation_context: str) -> Dict[str, Any]:
        """Generate enhanced response with all analysis components."""

        # ðŸš¨ EMERGENCY BYPASS: Check immediately if we should generate questions to avoid hanging V1/V2 calls
        if self._should_use_emergency_bypass(context_analysis, intent_analysis, business_validation, messages):
            logger.info("ðŸš¨ EMERGENCY BYPASS ACTIVATED IN _generate_response_enhanced: Generating questions immediately")
            return self._create_emergency_questions_response(
                context_analysis, stakeholder_analysis, intent_analysis, business_validation,
                industry_analysis, conversation_flow
            )

        try:
            # Import proven V1/V2 functions
            from backend.api.routes.customer_research import (
                generate_research_response_with_retry,
                generate_research_questions,
                generate_contextual_suggestions
            )

            llm_client = self._get_llm_client()

            # Convert dict messages to Message objects for V1/V2 compatibility
            from backend.api.routes.customer_research import Message as V1Message
            v1_messages = []
            for msg in messages:
                if isinstance(msg, dict):
                    v1_messages.append(V1Message(
                        id=msg.get('id', f'msg_{int(time.time())}'),
                        content=msg.get('content', ''),
                        role=msg.get('role', 'user'),
                        timestamp=msg.get('timestamp', datetime.now().isoformat()),
                        metadata=msg.get('metadata')
                    ))
                else:
                    v1_messages.append(msg)

            # Create context object for V1 compatibility
            context_obj = type('Context', (), {
                'businessIdea': context_analysis.get('business_idea'),
                'targetCustomer': context_analysis.get('target_customer'),
                'problem': context_analysis.get('problem')
            })()

            # Generate main response
            response_content = await generate_research_response_with_retry(
                llm_service=llm_client,
                messages=v1_messages,
                user_input=latest_input,
                context=context_obj,
                conversation_context=conversation_context
            )

            # Use proven V1/V2 confirmation and question generation logic
            from backend.api.routes.customer_research import (
                should_confirm_before_questions,
                should_generate_research_questions,
                generate_confirmation_response
            )

            # Check if we should confirm before generating questions (V1/V2 logic)
            should_confirm = should_confirm_before_questions(
                messages=v1_messages,
                user_input=latest_input,
                conversation_context=conversation_context
            )

            # Check if user confirmed and we should generate questions (V3 enhanced logic)
            # Use V3 business validation result with intelligent dialogue flow management
            v3_ready = business_validation.get('ready_for_questions', False) if business_validation else False

            # Intelligent conversation depth check - minimum 2, but can continue indefinitely if context unclear
            conversation_depth = len([msg for msg in v1_messages if msg.role == 'user'])
            min_conversation_depth = 2  # Minimum 2 user messages, but not a hard limit

            # Check for explicit user confirmation signals
            confirmation_phrases = [
                'yes', 'correct', 'right', 'exactly', 'that\'s it', 'that\'s right',
                'generate questions', 'ready for questions', 'create questions',
                'let\'s proceed', 'sounds good', 'perfect', 'that\'s accurate'
            ]
            user_confirmed_explicitly = any(
                phrase in latest_input.lower()
                for phrase in confirmation_phrases
            )

            # Check business context completeness from V3 validation
            business_clarity = business_validation.get('business_clarity', {}) if business_validation else {}
            idea_clarity = business_clarity.get('idea_clarity', 0)
            customer_clarity = business_clarity.get('customer_clarity', 0)
            problem_clarity = business_clarity.get('problem_clarity', 0)

            # Context is sufficiently clear if all aspects are above threshold
            context_sufficiently_clear = (
                idea_clarity >= 0.8 and
                customer_clarity >= 0.8 and
                problem_clarity >= 0.8
            )

            # Enhanced decision logic - flexible and intelligent
            should_generate_questions = (
                v3_ready and  # V3 system says ready
                conversation_depth >= min_conversation_depth and  # Minimum conversation depth
                user_confirmed_explicitly and  # User explicitly confirmed
                context_sufficiently_clear  # Context is clear enough
            )

            # EMERGENCY BYPASS: If questions should be generated, create them immediately
            # This bypasses the hanging response generation system
            if should_generate_questions:
                logger.info("ðŸš¨ EMERGENCY BYPASS: Creating questions immediately to avoid hanging V1/V2 calls")

                # Extract context for questions
                business_idea = context_analysis.get('business_idea') or context_analysis.get('businessIdea', 'your business')
                target_customer = context_analysis.get('target_customer') or context_analysis.get('targetCustomer', 'customers')
                problem = context_analysis.get('problem', 'challenges they face')

                # Create immediate working questions
                emergency_questions = {
                    "primaryStakeholders": [
                        {
                            "name": f"{target_customer.title()}",
                            "description": f"The primary users of the {business_idea}",
                            "questions": {
                                "problemDiscovery": [
                                    f"What challenges do you currently face with {business_idea.split()[-1] if business_idea else 'this service'}?",
                                    f"How do you currently handle {problem.split('.')[0] if problem else 'these needs'}?",
                                    "What's the most frustrating part of your current situation?",
                                    "How often do you encounter these problems?",
                                    "What would make this easier for you?"
                                ],
                                "solutionValidation": [
                                    f"Would a {business_idea} help solve your problem?",
                                    "What features would be most important to you?",
                                    "How much would you be willing to pay for this service?",
                                    "What would convince you to try this service?",
                                    "What concerns would you have about using this?"
                                ],
                                "followUp": [
                                    "Would you recommend this to others in your situation?",
                                    "What else should we know about your needs?",
                                    "Any other feedback or suggestions?"
                                ]
                            }
                        }
                    ],
                    "secondaryStakeholders": [
                        {
                            "name": "Family Members",
                            "description": f"People who care about or help {target_customer}",
                            "questions": {
                                "problemDiscovery": [
                                    f"How do you currently help {target_customer} with their needs?",
                                    "What challenges do you see them facing?",
                                    "How does this affect you or your family?"
                                ],
                                "solutionValidation": [
                                    f"Would you support using a {business_idea}?",
                                    "What would you want to see in this service?",
                                    "What concerns would you have?"
                                ],
                                "followUp": [
                                    "Would you help them access this service?",
                                    "Any other thoughts?"
                                ]
                            }
                        }
                    ],
                    "timeEstimate": {
                        "totalQuestions": 18,
                        "estimatedMinutes": "36-54",
                        "breakdown": {"primary": 13, "secondary": 5}
                    }
                }

                # Return immediate response with questions
                return {
                    "content": f"Perfect! I've generated comprehensive research questions for your {business_idea}. These questions will help you validate the market need and refine your solution.",
                    "questions": emergency_questions,
                    "suggestions": [],
                    "metadata": {
                        "suggestions": [],
                        "contextual_suggestions": [],
                        "extracted_context": context_analysis,
                        "user_intent": intent_analysis,
                        "business_readiness": business_validation,
                        "industry_analysis": industry_analysis,
                        "stakeholder_analysis": stakeholder_analysis,
                        "conversation_flow": conversation_flow,
                        "request_id": self.request_id,
                        "emergency_bypass": True,
                        "questionCategory": "comprehensive",
                        "researchStage": "questions_generated",
                        "conversation_stage": "questions_ready",
                        "show_confirmation": False,
                        "questions_generated": True,
                        "workflow_version": "v3_simple_emergency_bypass"
                    }
                }

            # Log the decision with detailed reasoning
            logger.info(f"Question generation decision: should_generate_questions={should_generate_questions}")
            logger.info(f"  - V3 ready: {v3_ready}")
            logger.info(f"  - Conversation depth: {conversation_depth} (min: {min_conversation_depth})")
            logger.info(f"  - User confirmed explicitly: {user_confirmed_explicitly}")
            logger.info(f"  - Context clarity: idea={idea_clarity:.2f}, customer={customer_clarity:.2f}, problem={problem_clarity:.2f}")
            logger.info(f"  - Context sufficiently clear: {context_sufficiently_clear}")

            # Check if emergency bypass was triggered
            if should_generate_questions:
                logger.info("ðŸš¨ EMERGENCY BYPASS WAS TRIGGERED - Questions should be generated immediately")
                logger.info("ðŸš¨ This should have returned early and avoided hanging V1/V2 calls")

            # Fallback to V1/V2 logic if V3 validation is not available
            if business_validation is None:
                logger.info("V3 business validation not available, falling back to V1/V2 logic")
                should_generate_questions = should_generate_research_questions(
                    messages=v1_messages,
                    user_input=latest_input,
                    conversation_context=conversation_context
                )

            # Generate confirmation response if needed (V1/V2 logic)
            if should_confirm and not should_generate_questions:
                response_content = await generate_confirmation_response(
                    llm_service=llm_client,
                    messages=v1_messages,
                    user_input=latest_input,
                    context=None,
                    conversation_context=conversation_context
                )

            questions = None

            # Generate comprehensive questions with stakeholder integration
            if should_generate_questions:
                # Use Instructor like the working pattern processor
                logger.info("ðŸš€ Using Instructor for question generation (following pattern processor approach)")

                try:
                    from backend.models.comprehensive_questions import ComprehensiveQuestions
                    from backend.services.llm.instructor_gemini_client import InstructorGeminiClient

                    # Create proper context from extracted analysis
                    # Try both field name formats for compatibility
                    business_idea = context_analysis.get('business_idea') or context_analysis.get('businessIdea')
                    target_customer = context_analysis.get('target_customer') or context_analysis.get('targetCustomer')
                    problem = context_analysis.get('problem')

                    logger.info(f"Creating research context: business_idea='{business_idea}', target_customer='{target_customer}', problem='{problem}'")

                    # Extract stakeholder information for prompt
                    primary_stakeholders = []
                    secondary_stakeholders = []

                    if stakeholder_analysis:
                        primary_stakeholders = stakeholder_analysis.get('primary', [])
                        secondary_stakeholders = stakeholder_analysis.get('secondary', [])

                    primary_names = [s.get('name', s) if isinstance(s, dict) else s for s in primary_stakeholders]
                    secondary_names = [s.get('name', s) if isinstance(s, dict) else s for s in secondary_stakeholders]

                    # Create optimized prompt for Instructor structured output
                    prompt = f"""Generate comprehensive customer research questions for this business:

BUSINESS CONTEXT:
- Business Idea: {business_idea}
- Target Customer: {target_customer}
- Problem: {problem}

STAKEHOLDERS TO CREATE QUESTIONS FOR:
Primary: {', '.join(primary_names) if primary_names else target_customer}
Secondary: {', '.join(secondary_names) if secondary_names else 'Family Members'}

REQUIREMENTS:
1. Create 5 problem discovery questions per stakeholder (understand current challenges)
2. Create 5 solution validation questions per stakeholder (test the proposed solution)
3. Create 3 follow-up questions per stakeholder (gather additional insights)
4. Make questions specific to each stakeholder's perspective and role
5. Use the exact business terminology and context provided
6. Ensure questions are actionable and will provide valuable insights
7. Avoid generic questions - make them specific to this business situation

Focus on creating questions that will help validate the market need and refine the solution for each stakeholder group."""

                    # Create Instructor client
                    instructor_client = InstructorGeminiClient()

                    # Generate structured output using Instructor (following pattern processor approach)
                    comprehensive_questions = await instructor_client.generate_with_model_async(
                        prompt=prompt,
                        model_class=ComprehensiveQuestions,
                        temperature=0.0,  # Use deterministic output like pattern processor
                        system_instruction="You are an expert customer research consultant. Generate comprehensive, specific research questions tailored to each stakeholder group.",
                        response_mime_type="application/json",  # Force JSON output like pattern processor
                        max_output_tokens=32000  # Keep high token limit for comprehensive output
                    )

                    logger.info(f"âœ… Instructor generated comprehensive questions successfully")
                    logger.info(f"Generated {len(comprehensive_questions.primaryStakeholders)} primary and {len(comprehensive_questions.secondaryStakeholders)} secondary stakeholders")

                    # Calculate and update time estimates
                    total_questions = comprehensive_questions.get_total_questions()
                    min_time, max_time = comprehensive_questions.get_estimated_time_range()

                    # Update the time estimate with calculated values
                    comprehensive_questions.timeEstimate.totalQuestions = total_questions
                    comprehensive_questions.timeEstimate.estimatedMinutes = f"{min_time}-{max_time}"
                    comprehensive_questions.timeEstimate.breakdown = {
                        'primary': sum(len(s.questions.problemDiscovery) + len(s.questions.solutionValidation) + len(s.questions.followUp)
                                      for s in comprehensive_questions.primaryStakeholders),
                        'secondary': sum(len(s.questions.problemDiscovery) + len(s.questions.solutionValidation) + len(s.questions.followUp)
                                        for s in comprehensive_questions.secondaryStakeholders),
                        'baseTime': min_time,
                        'withBuffer': max_time,
                        'perQuestion': 2.5
                    }

                    # Convert to dict for API compatibility
                    questions = comprehensive_questions.dict()

                    if questions and isinstance(questions, dict):
                        primary_count = len(questions.get('primaryStakeholders', []))
                        secondary_count = len(questions.get('secondaryStakeholders', []))
                        total_questions = questions.get('timeEstimate', {}).get('totalQuestions', 0)
                        logger.info(f"âœ… Successfully generated questions using Instructor: {primary_count} primary stakeholders, {secondary_count} secondary stakeholders, {total_questions} total questions")
                    else:
                        logger.warning("âš ï¸ Instructor returned invalid format")
                        questions = None

                    # Log comprehensive questions format (if questions were generated)
                    if questions and isinstance(questions, dict):
                        primary_count = len(questions.get('primaryStakeholders', []))
                        secondary_count = len(questions.get('secondaryStakeholders', []))
                        total_questions = questions.get('timeEstimate', {}).get('totalQuestions', 0)
                        estimated_time = questions.get('timeEstimate', {}).get('estimatedMinutes', '0-0')

                        logger.info(f"Generated comprehensive questions: {primary_count} primary stakeholders, {secondary_count} secondary stakeholders, {total_questions} total questions, estimated time: {estimated_time} minutes")
                except Exception as e:
                    logger.warning(f"Instructor question generation failed: {e}")
                    logger.info("Attempting to use emergency question generation...")

                    # Emergency fallback - create immediate working questions
                    try:
                        logger.info("ðŸ†˜ Using emergency question generation...")

                        # Extract context for questions
                        business_idea = business_idea or 'your business'
                        target_customer = target_customer or 'customers'
                        problem = problem or 'challenges they face'

                        questions = {
                            "primaryStakeholders": [
                                {
                                    "name": f"{target_customer.title()}",
                                    "description": f"The primary users of the {business_idea}",
                                    "questions": {
                                        "problemDiscovery": [
                                            f"What challenges do you currently face with {business_idea.split()[-1] if business_idea else 'this service'}?",
                                            f"How do you currently handle {problem.split('.')[0] if problem else 'these needs'}?",
                                            "What's the most frustrating part of your current situation?",
                                            "How often do you encounter these problems?",
                                            "What would make this easier for you?"
                                        ],
                                        "solutionValidation": [
                                            f"Would a {business_idea} help solve your problem?",
                                            "What features would be most important to you?",
                                            "How much would you be willing to pay for this service?",
                                            "What would convince you to try this service?",
                                            "What concerns would you have about using this?"
                                        ],
                                        "followUp": [
                                            "Would you recommend this to others in your situation?",
                                            "What else should we know about your needs?",
                                            "Any other feedback or suggestions?"
                                        ]
                                    }
                                }
                            ],
                            "secondaryStakeholders": [
                                {
                                    "name": "Family Members",
                                    "description": f"People who care about or help {target_customer}",
                                    "questions": {
                                        "problemDiscovery": [
                                            f"How do you currently help {target_customer} with their needs?",
                                            "What challenges do you see them facing?",
                                            "How does this affect you or your family?"
                                        ],
                                        "solutionValidation": [
                                            f"Would you support using a {business_idea}?",
                                            "What would you want to see in this service?",
                                            "What concerns would you have?"
                                        ],
                                        "followUp": [
                                            "Would you help them access this service?",
                                            "Any other thoughts?"
                                        ]
                                    }
                                }
                            ],
                            "timeEstimate": {
                                "totalQuestions": 18,
                                "estimatedMinutes": "36-54",
                                "breakdown": {"primary": 13, "secondary": 5}
                            }
                        }

                        if questions and isinstance(questions, dict):
                            primary_count = len(questions.get('primaryStakeholders', []))
                            secondary_count = len(questions.get('secondaryStakeholders', []))
                            total_questions = questions.get('timeEstimate', {}).get('totalQuestions', 0)
                            logger.info(f"âœ… Successfully using emergency questions: {primary_count} primary stakeholders, {secondary_count} secondary stakeholders, {total_questions} total questions")
                        else:
                            logger.warning("âš ï¸ Emergency questions returned invalid format, creating minimal questions")
                            # Create minimal questions as last resort
                            questions = {
                                "primaryStakeholders": [
                                    {
                                        "name": "Target Customers",
                                        "description": f"The primary users of the {business_idea}",
                                        "questions": {
                                            "problemDiscovery": [
                                                f"What challenges do you face with {business_idea.split()[-1] if business_idea else 'this service'}?",
                                                "How do you currently handle this need?",
                                                "What would make this easier for you?"
                                            ],
                                            "solutionValidation": [
                                                f"Would a {business_idea} help solve your problem?",
                                                "What features would be most important?",
                                                "How much would you pay for this?"
                                            ],
                                            "followUp": [
                                                "Any other thoughts?",
                                                "Would you recommend this to others?"
                                            ]
                                        }
                                    }
                                ],
                                "secondaryStakeholders": [],
                                "timeEstimate": {
                                    "totalQuestions": 8,
                                    "estimatedMinutes": "16-24",
                                    "breakdown": {"primary": 8, "secondary": 0}
                                }
                            }
                            logger.info("âœ… Created minimal fallback questions")

                    except Exception as fallback_error:
                        logger.error(f"Comprehensive fallback also failed: {fallback_error}")
                        import traceback
                        logger.error(f"Fallback error traceback: {traceback.format_exc()}")

                        # Ultimate fallback - create very basic questions
                        logger.info("ðŸ†˜ Using ultimate fallback - creating basic questions")
                        questions = {
                            "primaryStakeholders": [
                                {
                                    "name": "Users",
                                    "description": "Primary users of the service",
                                    "questions": {
                                        "problemDiscovery": [
                                            "What problem does this solve for you?",
                                            "How do you handle this currently?",
                                            "What's most frustrating about the current situation?"
                                        ],
                                        "solutionValidation": [
                                            "Would this solution help you?",
                                            "What would you want to see in this service?",
                                            "How much would you pay?"
                                        ],
                                        "followUp": [
                                            "Any other feedback?",
                                            "Would you use this service?"
                                        ]
                                    }
                                }
                            ],
                            "secondaryStakeholders": [],
                            "timeEstimate": {
                                "totalQuestions": 8,
                                "estimatedMinutes": "16-24",
                                "breakdown": {"primary": 8, "secondary": 0}
                            }
                        }
                        logger.info("âœ… Ultimate fallback questions created")

                # Log final questions status
                if questions:
                    logger.info(f"âœ… Successfully generated questions: {type(questions)}")
                    if isinstance(questions, dict):
                        if 'primaryStakeholders' in questions:
                            primary_count = len(questions.get('primaryStakeholders', []))
                            secondary_count = len(questions.get('secondaryStakeholders', []))
                            logger.info(f"âœ… Comprehensive format: {primary_count} primary, {secondary_count} secondary stakeholders")
                        else:
                            logger.info(f"âœ… Legacy format: {questions.keys()}")
                else:
                    logger.warning("âŒ No questions generated - will proceed without questions")

            # Generate contextual suggestions using proven V1/V2 LLM-based approach
            suggestions = []
            if not questions:  # Only generate suggestions if not generating questions (V1/V2 logic)
                try:
                    from backend.api.routes.customer_research import generate_contextual_suggestions, generate_fallback_suggestions

                    # Extract the actual response content string
                    assistant_response_text = response_content if isinstance(response_content, str) else response_content.get("content", "")

                    # Use the proven V1/V2 LLM-based suggestion generation
                    base_suggestions = await generate_contextual_suggestions(
                        llm_service=llm_client,
                        messages=v1_messages,
                        user_input=latest_input,
                        assistant_response=assistant_response_text,
                        conversation_context=conversation_context
                    )
                    logger.info(f"Generated V1/V2 LLM-based base suggestions: {base_suggestions}")

                    # Add UX research methodology special options as specified in requirements
                    if base_suggestions and len(base_suggestions) >= 1:
                        # Add special options at the beginning for discovery phases
                        suggestions = ["All of the above", "I don't know"] + base_suggestions
                        logger.info(f"âœ… Added special options to V1 suggestions: {suggestions}")
                    else:
                        # Fallback with special options
                        suggestions = ["All of the above", "I don't know", "Tell me more", "Continue", "What else?"]
                        logger.info(f"âœ… Using fallback suggestions with special options: {suggestions}")

                    # Ensure we have 3-5 suggestions as specified in requirements
                    if len(suggestions) > 5:
                        suggestions = suggestions[:5]

                except Exception as e:
                    logger.warning(f"V1/V2 LLM suggestion generation failed: {e}")
                    # Use contextual fallback suggestions based on business context
                    assistant_response_text = response_content if isinstance(response_content, str) else response_content.get("content", "")

                    # Generate contextual suggestions based on extracted context
                    # Try both camelCase and snake_case field names for compatibility
                    business_idea = (context_analysis.get('business_idea') or context_analysis.get('businessIdea', '')) if context_analysis else ''
                    target_customer = (context_analysis.get('target_customer') or context_analysis.get('targetCustomer', '')) if context_analysis else ''

                    if business_idea and 'laundry' in business_idea.lower():
                        base_suggestions = [
                            "Pick-up and delivery service",
                            "Self-service laundromat",
                            "Commercial clients focus"
                        ]
                        # Add special options for UX research methodology
                        suggestions = ["All of the above", "I don't know"] + base_suggestions
                    elif business_idea and target_customer:
                        base_suggestions = [
                            f"Tell me more about {target_customer}",
                            f"What challenges do {target_customer} face?",
                            f"How would {business_idea} help them?"
                        ]
                        # Add special options for UX research methodology
                        suggestions = ["All of the above", "I don't know"] + base_suggestions
                    else:
                        # Use V1/V2 fallback as last resort
                        base_suggestions = generate_fallback_suggestions(latest_input, assistant_response_text)
                        # Add special options for UX research methodology
                        suggestions = ["All of the above", "I don't know"] + base_suggestions

                    # Ensure we have 3-5 suggestions as specified in requirements
                    if len(suggestions) > 5:
                        suggestions = suggestions[:5]

                    logger.info(f"Using contextual fallback suggestions: {suggestions}")

            # Ensure we return the correct content format
            content_text = response_content if isinstance(response_content, str) else response_content.get("content", "")

            return {
                "content": content_text,
                "questions": questions,
                "suggestions": suggestions,
                "metadata": {
                    "suggestions": suggestions,
                    "contextual_suggestions": suggestions,
                    "extracted_context": context_analysis,
                    "user_intent": intent_analysis,
                    "business_readiness": business_validation,
                    "industry_analysis": industry_analysis,
                    "stakeholder_analysis": stakeholder_analysis,
                    "conversation_flow": conversation_flow,
                    "request_id": self.request_id
                }
            }

        except Exception as e:
            logger.error(f"Response generation failed: {e}")
            # Fallback response with UX research methodology special options
            fallback_suggestions = ["All of the above", "I don't know", "Tell me more", "That sounds interesting", "What else?"]
            return {
                "content": "I understand you're working on your business idea. Could you tell me more about what you're trying to build?",
                "questions": None,
                "suggestions": fallback_suggestions,
                "metadata": {
                    "suggestions": fallback_suggestions,
                    "contextual_suggestions": fallback_suggestions,
                    "error": "response_generation_failed",
                    "request_id": self.request_id
                }
            }

    async def _fallback_to_v1_analysis(self, conversation_context: str, latest_input: str,
                                     messages: List[Dict[str, Any]], existing_context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Fallback to V1 analysis when V3 enhanced analysis fails."""

        logger.info("Using V1 fallback analysis")
        self.metrics.fallback_used = True

        try:
            # Import proven V1 functions
            from backend.api.routes.customer_research import (
                generate_research_response_with_retry,
                analyze_user_intent_with_llm,
                validate_business_readiness_with_llm,
                extract_context_with_llm
            )

            llm_client = self._get_llm_client()

            # Basic V1 analysis
            context_obj = type('Context', (), {
                'businessIdea': existing_context.get('businessIdea') if existing_context else None,
                'targetCustomer': existing_context.get('targetCustomer') if existing_context else None,
                'problem': existing_context.get('problem') if existing_context else None,
                'stage': existing_context.get('stage') if existing_context else None
            })()

            # Extract context
            context_analysis = await extract_context_with_llm(
                llm_service=llm_client,
                conversation_context=conversation_context,
                latest_input=latest_input
            )

            # Convert dict messages to Message objects for V1/V2 compatibility
            from backend.api.routes.customer_research import Message as V1Message
            v1_messages = []
            for msg in messages:
                if isinstance(msg, dict):
                    v1_messages.append(V1Message(
                        id=msg.get('id', f'msg_{int(time.time())}'),
                        content=msg.get('content', ''),
                        role=msg.get('role', 'user'),
                        timestamp=msg.get('timestamp', datetime.now().isoformat()),
                        metadata=msg.get('metadata')
                    ))
                else:
                    v1_messages.append(msg)

            # Analyze intent
            intent_analysis = await analyze_user_intent_with_llm(
                llm_service=llm_client,
                conversation_context=conversation_context,
                latest_input=latest_input,
                messages=v1_messages
            )

            # Validate business readiness
            business_validation = await validate_business_readiness_with_llm(
                llm_service=llm_client,
                conversation_context=conversation_context,
                latest_input=latest_input
            )

            # Generate response
            response_content = await generate_research_response_with_retry(
                llm_service=llm_client,
                messages=v1_messages,
                user_input=latest_input,
                context=context_obj,
                conversation_context=conversation_context
            )

            # Generate contextual suggestions using V1/V2 LLM-based approach
            suggestions = []
            try:
                from backend.api.routes.customer_research import generate_contextual_suggestions, generate_fallback_suggestions

                # Use the proven V1/V2 LLM-based suggestion generation
                base_suggestions = await generate_contextual_suggestions(
                    llm_service=llm_client,
                    messages=v1_messages,
                    user_input=latest_input,
                    assistant_response=response_content,
                    conversation_context=conversation_context
                )
                logger.info(f"V1 fallback generated LLM-based base suggestions: {base_suggestions}")

                # Add UX research methodology special options as specified in requirements
                if base_suggestions and len(base_suggestions) >= 1:
                    # Add special options at the beginning for discovery phases
                    suggestions = ["All of the above", "I don't know"] + base_suggestions
                    logger.info(f"âœ… V1 fallback: Added special options to suggestions: {suggestions}")
                else:
                    # Fallback with special options
                    suggestions = ["All of the above", "I don't know", "Tell me more", "Continue", "What else?"]
                    logger.info(f"âœ… V1 fallback: Using fallback suggestions with special options: {suggestions}")

                # Ensure we have 3-5 suggestions as specified in requirements
                if len(suggestions) > 5:
                    suggestions = suggestions[:5]

            except Exception as e:
                logger.warning(f"V1 fallback LLM suggestion generation failed: {e}")
                # Use contextual fallback suggestions based on extracted context
                business_idea = context_analysis.get('businessIdea', '')
                target_customer = context_analysis.get('targetCustomer', '')

                if business_idea and 'laundry' in business_idea.lower():
                    base_suggestions = [
                        "Pick-up and delivery service",
                        "Self-service laundromat",
                        "Commercial clients focus"
                    ]
                    # Add special options for UX research methodology
                    suggestions = ["All of the above", "I don't know"] + base_suggestions
                elif business_idea and target_customer:
                    base_suggestions = [
                        f"Tell me more about {target_customer}",
                        f"What challenges do {target_customer} face?",
                        f"How would {business_idea} help them?"
                    ]
                    # Add special options for UX research methodology
                    suggestions = ["All of the above", "I don't know"] + base_suggestions
                else:
                    base_suggestions = generate_fallback_suggestions(latest_input, response_content)
                    # Add special options for UX research methodology
                    suggestions = ["All of the above", "I don't know"] + base_suggestions

                # Ensure we have 3-5 suggestions as specified in requirements
                if len(suggestions) > 5:
                    suggestions = suggestions[:5]

            # Add fallback thinking step
            if self.config.enable_thinking_process:
                self._add_thinking_step("V1 Fallback", "completed", f"Successfully used V1 fallback analysis with {len(suggestions)} contextual suggestions")

            return {
                "content": response_content,
                "suggestions": suggestions,
                "metadata": {
                    "extracted_context": context_analysis,
                    "user_intent": intent_analysis,
                    "business_readiness": business_validation,
                    "fallback_used": True,
                    "request_id": self.request_id,
                    "suggestions": suggestions,
                    "contextual_suggestions": suggestions
                },
                "questions": None,
                "thinking_process": self.thinking_steps if self.config.enable_thinking_process else [],
                "performance_metrics": {
                    "total_duration_ms": self.metrics.total_duration_ms,
                    "fallback_used": True
                }
            }

        except Exception as e:
            logger.error(f"V1 fallback analysis also failed: {e}")

            # Ultimate fallback
            return {
                "content": "I'm having some technical difficulties right now. Could you please try again in a moment?",
                "metadata": {"error": "fallback_failed", "request_id": self.request_id},
                "questions": None,
                "thinking_process": self.thinking_steps if self.config.enable_thinking_process else [],
                "performance_metrics": {"total_duration_ms": self.metrics.total_duration_ms, "error": True}
            }


# API Endpoints

@router.post("/chat")
async def chat_v3_simple(
    request: ChatRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    V3 Simplified customer research chat endpoint.

    This endpoint provides all V3 enhanced features with V1/V2 stability:
    - Enhanced context extraction and analysis
    - Industry classification and stakeholder detection
    - Intelligent conversation flow management
    - Smart caching and performance optimization
    - Comprehensive thinking process tracking
    - Robust error handling with V1 fallback
    """

    try:
        logger.info(f"V3 Simple chat request from user {request.user_id if request.user_id else 'anonymous'}")

        # Validate request (optional - skip if validation module not available)
        try:
            from backend.utils.research_validation import validate_research_request, ValidationError as ResearchValidationError
            validate_research_request(request.model_dump())
        except ImportError:
            logger.debug("Research validation module not available - skipping validation")
        except Exception as e:
            logger.warning(f"Request validation warning: {e}")
            # Continue with warning for research chat

        # Create request-scoped service
        config = SimplifiedConfig(
            enable_thinking_process=request.enable_thinking_process
        )

        # Override enhanced analysis features based on request
        if not request.enable_enhanced_analysis:
            config.enable_industry_analysis = False
            config.enable_stakeholder_detection = False
            config.enable_enhanced_context = False
        service = SimplifiedResearchService(config)

        # Build conversation context with validation
        conversation_context = ""
        try:
            for msg in request.messages:
                if msg and msg.role and msg.content:
                    conversation_context += f"{msg.role}: {msg.content}\n"

            # Add current user input
            if request.input:
                conversation_context += f"user: {request.input}\n"

            # Ensure we have some content
            if not conversation_context.strip():
                conversation_context = f"user: {request.input or 'Hello'}\n"

        except Exception as e:
            logger.warning(f"Error building conversation context: {e}")
            conversation_context = f"user: {request.input or 'Hello'}\n"

        logger.info(f"Processing conversation context: {len(conversation_context)} characters")
        logger.debug(f"Conversation context preview: {conversation_context[:200]}...")

        # Perform comprehensive analysis
        analysis_result = await service.analyze_comprehensive(
            conversation_context=conversation_context,
            latest_input=request.input,
            messages=[msg.model_dump() for msg in request.messages],
            existing_context=request.context.model_dump() if request.context else None
        )

        # Build response with proper metadata including suggestions
        metadata = analysis_result.get("metadata", {})

        # Ensure suggestions are in metadata if they exist in the analysis result
        if "suggestions" in analysis_result and analysis_result["suggestions"]:
            metadata["suggestions"] = analysis_result["suggestions"]
            metadata["contextual_suggestions"] = analysis_result["suggestions"]

        # Ensure request_id is in metadata for progressive updates
        metadata["request_id"] = service.request_id

        # Return plain dictionary to avoid Pydantic validation issues
        response = {
            "content": analysis_result["content"],
            "metadata": metadata,
            "questions": analysis_result.get("questions"),
            "session_id": request.session_id,
            "thinking_process": analysis_result.get("thinking_process", []),
            "performance_metrics": analysis_result.get("performance_metrics"),
            "api_version": "v3-simple"
        }

        logger.info(f"V3 Simple chat completed successfully in {analysis_result.get('performance_metrics', {}).get('total_duration_ms', 0)}ms")

        # Clean up instance from registry after completion
        if hasattr(service, 'cleanup'):
            service.cleanup()

        return response

    except Exception as e:
        logger.error(f"V3 Simple chat endpoint error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Chat analysis failed: {str(e)}")


@router.get("/thinking-progress/{request_id}")
async def get_thinking_progress(request_id: str):
    """Get current thinking process steps for progressive updates."""
    try:
        logger.debug(f"Polling thinking progress for request_id: {request_id}")
        logger.debug(f"Active instances: {list(SimplifiedResearchService._active_instances.keys())}")

        if request_id in SimplifiedResearchService._active_instances:
            service = SimplifiedResearchService._active_instances[request_id]
            current_steps = service.get_current_thinking_steps()

            logger.debug(f"Found active instance {request_id} with {len(current_steps)} steps")

            return {
                "request_id": request_id,
                "thinking_steps": current_steps,
                "total_steps": len(current_steps),
                "completed_steps": len([s for s in current_steps if s.get("status") == "completed"]),
                "is_active": True
            }
        else:
            logger.debug(f"Request ID {request_id} not found in active instances")
            return {
                "request_id": request_id,
                "thinking_steps": [],
                "total_steps": 0,
                "completed_steps": 0,
                "is_active": False
            }
    except Exception as e:
        logger.error(f"Error getting thinking progress for {request_id}: {e}")
        return {
            "request_id": request_id,
            "thinking_steps": [],
            "total_steps": 0,
            "completed_steps": 0,
            "is_active": False,
            "error": str(e)
        }

@router.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint for V3 Simplified API."""

    try:
        # Test service initialization
        service = SimplifiedResearchService()

        return HealthResponse(
            status="healthy",
            version="v3-simple",
            features=[
                "enhanced_context_analysis",
                "intelligent_intent_detection",
                "business_readiness_validation",
                "industry_classification",
                "stakeholder_detection",
                "conversation_flow_management",
                "smart_caching",
                "thinking_process_tracking",
                "v1_fallback_support",
                "performance_monitoring"
            ],
            performance={
                "request_timeout_seconds": 30,
                "cache_enabled": True,
                "fallback_enabled": True
            },
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        logger.error(f"V3 Simple health check failed: {e}")
        return HealthResponse(
            status="degraded",
            version="v3-simple",
            features=[],
            performance={"error": str(e)},
            timestamp=datetime.now().isoformat()
        )


@router.post("/generate-questions", response_model=ResearchQuestions)
async def generate_questions_v3_simple(
    request: GenerateQuestionsRequest,
    db: Session = Depends(get_db)
):
    """
    Generate research questions based on context and conversation history.
    V3 Simple version with enhanced capabilities.
    """
    try:
        logger.info("Generating research questions (V3 Simple)")

        # Import V1/V2 proven function
        from backend.api.routes.customer_research import generate_research_questions
        from backend.services.llm import LLMServiceFactory

        # Create LLM service
        llm_service = LLMServiceFactory.create("gemini")

        # Generate questions using proven V1/V2 logic
        questions = await generate_research_questions(
            llm_service=llm_service,
            context=request.context,
            conversation_history=request.conversationHistory
        )

        return questions

    except Exception as e:
        logger.error(f"Error generating questions: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Question generation failed: {str(e)}")


@router.get("/sessions")
async def get_research_sessions_v3_simple(
    user_id: Optional[str] = None,
    limit: int = 20,
    db: Session = Depends(get_db)
):
    """Get research sessions for dashboard (V3 Simple version)."""
    try:
        # Import V1/V2 session service
        from backend.services.research_session_service import ResearchSessionService

        session_service = ResearchSessionService(db)

        if user_id:
            sessions = session_service.get_user_sessions(user_id, limit)
        else:
            sessions = session_service.get_recent_sessions(limit)

        # Convert to summary format
        summaries = []
        for session in sessions:
            summary = session_service.get_session_summary(session.session_id)
            if summary:
                summaries.append(summary)

        return summaries

    except Exception as e:
        logger.error(f"Error getting sessions: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get sessions: {str(e)}")

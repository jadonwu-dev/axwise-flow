from fastapi import HTTPException
from sqlalchemy.orm import Session
import json
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional, Literal
from sqlalchemy import desc, asc

from backend.models import User, InterviewData, AnalysisResult, Persona

# Configure logging
logger = logging.getLogger(__name__)

# Default sentiment values when none are available
DEFAULT_SENTIMENT_OVERVIEW = {"positive": 0.33, "neutral": 0.34, "negative": 0.33}


class ResultsService:
    """
    Service class for handling retrieval and formatting of analysis results.
    """

    def __init__(self, db: Session, user: User):
        """
        Initialize the ResultsService with database session and user.

        Args:
            db (Session): SQLAlchemy database session
            user (User): Current authenticated user
        """
        self.db = db
        self.user = user

    def get_analysis_result(self, result_id: int) -> Dict[str, Any]:
        """
        Retrieve a specific analysis result.

        Args:
            result_id: ID of the analysis result to retrieve

        Returns:
            Analysis result data formatted for API response

        Raises:
            HTTPException: If result not found or not accessible by user
        """
        try:
            logger.info(
                f"Retrieving results for result_id: {result_id}, user: {self.user.user_id}"
            )

            # Query for results with user authorization check
            analysis_result = (
                self.db.query(AnalysisResult)
                .join(InterviewData)
                .filter(
                    AnalysisResult.result_id == result_id,
                    InterviewData.user_id == self.user.user_id,
                )
                .first()
            )

            if not analysis_result:
                logger.error(
                    f"Results not found - result_id: {result_id}, user_id: {self.user.user_id}"
                )
                raise HTTPException(
                    status_code=404,
                    detail=f"Results not found for result_id: {result_id}",
                )

            # Check if results are available
            if not analysis_result.results:
                return {
                    "status": "processing",
                    "message": "Analysis is still in progress.",
                }

            # Check for error in results
            if (
                isinstance(analysis_result.results, dict)
                and "error" in analysis_result.results
            ):
                return {
                    "status": "error",
                    "result_id": analysis_result.result_id,
                    "error": analysis_result.results["error"],
                }

            # Parse stored results and format them
            try:
                # Parse stored results to Python dict
                results_dict = (
                    json.loads(analysis_result.results)
                    if isinstance(analysis_result.results, str)
                    else analysis_result.results
                )

                # Enhanced logging for personas debug
                logger.info(f"Results keys available: {list(results_dict.keys())}")

                # Ensure personas are present in the results dictionary
                # Using direct personas from results JSON instead of database
                self._ensure_personas_present(results_dict, result_id)

                # Get personas from the results JSON (current approach)
                persona_list = []
                if "personas" in results_dict and isinstance(
                    results_dict["personas"], list
                ):
                    from backend.schemas import PersonaTrait, Persona as PersonaSchema

                    # Log persona count for debugging
                    logger.info(
                        f"Found {len(results_dict['personas'])} personas in results JSON"
                    )

                    # Process each persona from JSON
                    for p_data in results_dict["personas"]:
                        try:
                            if not isinstance(p_data, dict):
                                logger.warning(
                                    f"Skipping non-dict persona data: {type(p_data)}"
                                )
                                continue

                            # Create proper persona schema object
                            persona = self._map_json_to_persona_schema(p_data)
                            persona_list.append(persona)

                            # Store this persona in the database for future use
                            # This won't affect current request but prepares for future schema changes
                            # Not blocking the request if this fails, just logging errors
                            try:
                                self._store_persona_in_db(p_data, result_id)
                            except Exception as store_err:
                                logger.error(
                                    f"Error storing persona in DB: {str(store_err)}"
                                )

                        except Exception as e:
                            logger.error(f"Error mapping persona from JSON: {str(e)}")

                # Create formatted response
                formatted_results = {
                    "status": "completed",
                    "result_id": analysis_result.result_id,
                    "analysis_date": analysis_result.analysis_date,
                    "results": {
                        "themes": results_dict.get("themes", []),
                        "enhanced_themes": results_dict.get(
                            "enhanced_themes", []
                        ),  # Include enhanced themes
                        "patterns": results_dict.get("patterns", []),
                        "sentiment": results_dict.get("sentiment", []),
                        "sentimentOverview": results_dict.get(
                            "sentimentOverview", DEFAULT_SENTIMENT_OVERVIEW
                        ),
                        "sentimentStatements": results_dict.get(
                            "sentimentStatements",
                            {"positive": [], "neutral": [], "negative": []},
                        ),
                        "insights": results_dict.get("insights", []),
                        "personas": persona_list,  # Use personas from the results JSON
                    },
                    "llm_provider": analysis_result.llm_provider,
                    "llm_model": analysis_result.llm_model,
                }

                # Log whether sentimentStatements were found
                has_sentiment_statements = "sentimentStatements" in results_dict
                logger.info(
                    f"SentimentStatements found in results_dict: {has_sentiment_statements}"
                )

                # If no sentiment statements are provided, extract them from themes and patterns
                sentimentStatements = formatted_results["results"][
                    "sentimentStatements"
                ]
                if (
                    not sentimentStatements["positive"]
                    and not sentimentStatements["neutral"]
                    and not sentimentStatements["negative"]
                ):
                    logger.info(
                        "No sentiment statements found, extracting from themes and patterns"
                    )

                    sentimentStatements = self._extract_sentiment_statements_from_data(
                        formatted_results["results"]["themes"],
                        formatted_results["results"]["patterns"],
                    )

                    formatted_results["results"][
                        "sentimentStatements"
                    ] = sentimentStatements
                    logger.info(
                        f"Extracted sentiment statements: positive={len(sentimentStatements['positive'])}, "
                        + f"neutral={len(sentimentStatements['neutral'])}, "
                        + f"negative={len(sentimentStatements['negative'])}"
                    )

                return formatted_results

            except (json.JSONDecodeError, AttributeError, KeyError) as e:
                logger.error(f"Error formatting results: {str(e)}")
                return {
                    "status": "error",
                    "result_id": analysis_result.result_id,
                    "error": f"Error formatting results: {str(e)}",
                }

        except HTTPException:
            # Re-raise HTTP exceptions
            raise
        except Exception as e:
            logger.error(f"Error retrieving results: {str(e)}")
            raise HTTPException(
                status_code=500, detail=f"Internal server error: {str(e)}"
            )

    def get_all_analyses(
        self,
        sort_by: Optional[str] = None,
        sort_direction: Optional[Literal["asc", "desc"]] = "desc",
        status: Optional[Literal["pending", "completed", "failed"]] = None,
    ) -> List[Dict[str, Any]]:
        """
        Retrieve all analyses for the current user.

        Args:
            sort_by: Field to sort by (createdAt, fileName)
            sort_direction: Sort direction (asc, desc)
            status: Filter by status

        Returns:
            List of formatted analysis results
        """
        try:
            # Log retrieval request
            logger.info(f"list_analyses called - user_id: {self.user.user_id}")
            logger.info(
                f"Request parameters - sortBy: {sort_by}, sortDirection: {sort_direction}, status: {status}"
            )

            # Build the query with user authorization check
            query = (
                self.db.query(AnalysisResult)
                .join(InterviewData)
                .filter(InterviewData.user_id == self.user.user_id)
            )

            # Apply status filter if provided
            if status:
                query = query.filter(AnalysisResult.status == status)

            # Apply sorting
            if sort_by == "createdAt" or sort_by is None:
                # Default sorting by creation date
                if sort_direction == "asc":
                    query = query.order_by(AnalysisResult.analysis_date.asc())
                else:
                    query = query.order_by(AnalysisResult.analysis_date.desc())
            elif sort_by == "fileName":
                # Sorting by filename requires joining with InterviewData
                if sort_direction == "asc":
                    query = query.order_by(InterviewData.filename.asc())
                else:
                    query = query.order_by(InterviewData.filename.desc())

            # Execute query
            analysis_results = query.all()

            # Format the results
            formatted_results = []
            for result in analysis_results:
                # Skip results with no data
                if not result or not result.interview_data:
                    continue

                # Format data to match frontend schema
                formatted_result = self._format_analysis_list_item(result)
                formatted_results.append(formatted_result)

            logger.info(
                f"Returning {len(formatted_results)} analyses for user {self.user.user_id}"
            )

            return formatted_results

        except Exception as e:
            logger.error(f"Error retrieving analyses: {str(e)}")
            raise HTTPException(
                status_code=500, detail=f"Internal server error: {str(e)}"
            )

    def _ensure_personas_present(
        self, results_dict: Dict[str, Any], result_id: int
    ) -> None:
        """
        Ensure personas are present in the results dictionary.
        Instead of querying the personas table, this extracts personas directly from the results JSON.

        Args:
            results_dict: Analysis results dictionary to modify
            result_id: ID of the analysis result
        """
        # Log the method call for debugging
        logger.info(f"_ensure_personas_present called with result_id: {result_id}")

        # Check if personas are already in the results dictionary
        if "personas" in results_dict and results_dict["personas"]:
            persona_count = len(results_dict.get("personas", []))
            logger.info(f"Found {persona_count} personas already in results dictionary")
            return  # Personas already exist in the results, no need to modify

        # No personas in the results dict, initialize empty array
        # Note: We're not adding mock personas anymore since we want real data only
        logger.info(
            f"No personas found in results dictionary for result_id: {result_id}"
        )
        results_dict["personas"] = []

    def _format_analysis_list_item(self, result: AnalysisResult) -> Dict[str, Any]:
        """
        Format a single analysis result for the list view.

        Args:
            result: AnalysisResult database record

        Returns:
            Formatted result for API response
        """
        # Format data to match frontend schema
        formatted_result = {
            "id": str(result.result_id),
            "status": result.status,
            "createdAt": result.analysis_date.isoformat(),
            "fileName": (
                result.interview_data.filename if result.interview_data else "Unknown"
            ),
            "fileSize": None,  # We don't store this currently
            "themes": [],
            "enhanced_themes": [],  # Initialize empty enhanced themes list
            "patterns": [],
            "sentimentOverview": DEFAULT_SENTIMENT_OVERVIEW,
            "sentiment": [],
            "personas": [],  # Initialize empty personas list
        }

        # Add results data if available
        if result.results:
            try:
                # Parse results data
                results_data = (
                    json.loads(result.results)
                    if isinstance(result.results, str)
                    else result.results
                )

                if isinstance(results_data, dict):
                    if "themes" in results_data and isinstance(
                        results_data["themes"], list
                    ):
                        formatted_result["themes"] = results_data["themes"]
                    if "enhanced_themes" in results_data and isinstance(
                        results_data["enhanced_themes"], list
                    ):
                        formatted_result["enhanced_themes"] = results_data[
                            "enhanced_themes"
                        ]
                    if "patterns" in results_data and isinstance(
                        results_data["patterns"], list
                    ):
                        formatted_result["patterns"] = results_data["patterns"]
                    if "sentimentOverview" in results_data and isinstance(
                        results_data["sentimentOverview"], dict
                    ):
                        formatted_result["sentimentOverview"] = results_data[
                            "sentimentOverview"
                        ]
                    if "sentiment" in results_data:
                        formatted_result["sentiment"] = (
                            results_data["sentiment"]
                            if isinstance(results_data["sentiment"], list)
                            else []
                        )
                    # Add personas if available
                    if "personas" in results_data:
                        formatted_result["personas"] = (
                            results_data["personas"]
                            if isinstance(results_data["personas"], list)
                            else []
                        )

                    # Add error info if available
                    if result.status == "failed" and "error" in results_data:
                        formatted_result["error"] = results_data["error"]
            except (json.JSONDecodeError, TypeError) as e:
                logger.error(f"Error parsing results data: {str(e)}")

        # Map API status to schema status values
        if result.status == "processing":
            formatted_result["status"] = "pending"  # Match schema requirements
        elif result.status == "error":
            formatted_result["status"] = "failed"  # Match schema requirements

        return formatted_result

    def _map_json_to_persona_schema(self, p_data: Dict[str, Any]):
        """
        Map JSON persona data to a proper PersonaSchema object

        Args:
            p_data: Persona data from JSON

        Returns:
            PersonaSchema object
        """
        from backend.schemas import PersonaTrait, Persona as PersonaSchema

        # Extract fields with safe fallbacks
        name = p_data.get("name", "Unknown")
        description = p_data.get("description", name)
        confidence = p_data.get("confidence", 0.5)
        patterns = p_data.get("patterns", [])
        evidence = p_data.get("evidence", [])
        metadata = p_data.get("metadata", {})

        # Extract nested traits or use empty defaults
        role_context_data = p_data.get("role_context", {})
        key_resp_data = p_data.get("key_responsibilities", {})
        tools_data = p_data.get("tools_used", {})
        collab_style_data = p_data.get("collaboration_style", {})
        analysis_approach_data = p_data.get("analysis_approach", {})
        pain_points_data = p_data.get("pain_points", {})

        # Create and return persona schema object
        persona = PersonaSchema(
            name=name,
            description=description,
            role_context=PersonaTrait(
                value=role_context_data.get("value", {}),
                confidence=role_context_data.get("confidence", confidence),
                evidence=role_context_data.get("evidence", evidence),
            ),
            key_responsibilities=PersonaTrait(
                value=key_resp_data.get("value", []),
                confidence=key_resp_data.get("confidence", confidence),
                evidence=key_resp_data.get("evidence", evidence),
            ),
            tools_used=PersonaTrait(
                value=tools_data.get("value", {}),
                confidence=tools_data.get("confidence", confidence),
                evidence=tools_data.get("evidence", evidence),
            ),
            collaboration_style=PersonaTrait(
                value=collab_style_data.get("value", {}),
                confidence=collab_style_data.get("confidence", confidence),
                evidence=collab_style_data.get("evidence", evidence),
            ),
            analysis_approach=PersonaTrait(
                value=analysis_approach_data.get("value", {}),
                confidence=analysis_approach_data.get("confidence", confidence),
                evidence=analysis_approach_data.get("evidence", evidence),
            ),
            pain_points=PersonaTrait(
                value=pain_points_data.get("value", []),
                confidence=pain_points_data.get("confidence", confidence),
                evidence=pain_points_data.get("evidence", evidence),
            ),
            patterns=patterns,
            confidence=confidence,
            evidence=evidence,
            metadata=metadata,
        )
        return persona

    def _store_persona_in_db(self, p_data: Dict[str, Any], result_id: int) -> None:
        """
        Store persona data in the database for future retrieval.
        This is a background process that doesn't affect the current request.

        Args:
            p_data: Persona data from JSON
            result_id: Analysis result ID
        """
        from backend.models import Persona

        # Extract fields with safe fallbacks
        name = p_data.get("name", "Unknown")
        description = p_data.get("description", "")
        archetype = p_data.get("archetype", None)

        # Check if this persona already exists in the database
        existing = (
            self.db.query(Persona)
            .filter(Persona.result_id == result_id, Persona.name == name)
            .first()
        )

        if existing:
            logger.info(
                f"Persona '{name}' already exists in database for result_id: {result_id}"
            )
            return

        # Extract data for DB fields - handle both new and legacy fields
        # For each field, try the new field name first, then fall back to legacy field name

        # Demographics (new) / role_context (legacy)
        demographics_data = p_data.get("demographics", p_data.get("role_context", {}))
        demographics = (
            demographics_data.get("value", {})
            if isinstance(demographics_data, dict)
            else {}
        )

        # Goals and motivations (new) / key_responsibilities (legacy)
        goals_data = p_data.get(
            "goals_and_motivations", p_data.get("key_responsibilities", {})
        )
        goals = goals_data.get("value", []) if isinstance(goals_data, dict) else []

        # Technology and tools (new) / tools_used (legacy)
        tools_data = p_data.get("technology_and_tools", p_data.get("tools_used", {}))
        tools = tools_data.get("value", {}) if isinstance(tools_data, dict) else {}

        # Challenges and frustrations (new) / pain_points (legacy)
        pain_points_data = p_data.get(
            "challenges_and_frustrations", p_data.get("pain_points", {})
        )
        pain_points_value = (
            pain_points_data.get("value", [])
            if isinstance(pain_points_data, dict)
            else []
        )

        # Extract other fields
        collab_style_data = p_data.get("collaboration_style", {})
        collab_style_value = (
            collab_style_data.get("value", {})
            if isinstance(collab_style_data, dict)
            else {}
        )

        analysis_approach_data = p_data.get(
            "attitude_towards_research", p_data.get("analysis_approach", {})
        )
        analysis_approach_value = (
            analysis_approach_data.get("value", {})
            if isinstance(analysis_approach_data, dict)
            else {}
        )

        # Get key quotes if available
        key_quotes_data = p_data.get("key_quotes", {})
        quotes = (
            key_quotes_data.get("value", [])
            if isinstance(key_quotes_data, dict)
            else []
        )

        # Get overall confidence - try both new and legacy field names
        confidence = p_data.get("overall_confidence", p_data.get("confidence", 0.5))

        # Get patterns and evidence
        patterns = p_data.get("patterns", [])
        evidence = p_data.get("supporting_evidence_summary", p_data.get("evidence", []))

        # Get metadata - try both new and legacy field names
        metadata = p_data.get("persona_metadata", p_data.get("metadata", {}))

        # Extract additional new fields if they exist
        skills_data = p_data.get("skills_and_expertise", {})
        skills = skills_data.get("value", {}) if isinstance(skills_data, dict) else {}

        workflow_data = p_data.get("workflow_and_environment", {})
        workflow = (
            workflow_data.get("value", {}) if isinstance(workflow_data, dict) else {}
        )

        needs_data = p_data.get("needs_and_desires", {})
        needs = needs_data.get("value", {}) if isinstance(needs_data, dict) else {}

        ai_attitude_data = p_data.get("attitude_towards_ai", {})
        ai_attitude = (
            ai_attitude_data.get("value", {})
            if isinstance(ai_attitude_data, dict)
            else {}
        )

        # Create new persona record with all available fields
        # Make sure we only use fields that actually exist in the SQLAlchemy model
        try:
            # Create a dictionary of fields to initialize the Persona object
            # This allows us to dynamically include or exclude fields based on what's available
            persona_fields = {
                "result_id": result_id,
                "name": name,
                "description": description,
                # Legacy fields that we know exist in all versions
                "role_context": json.dumps(demographics),
                "key_responsibilities": json.dumps(goals),
                "tools_used": json.dumps(tools),
                "collaboration_style": json.dumps(collab_style_value),
                "analysis_approach": json.dumps(analysis_approach_value),
                "pain_points": json.dumps(pain_points_value),
                "patterns": json.dumps(patterns),
                "confidence": confidence,  # Use the float directly
                "evidence": json.dumps(evidence),
                "persona_metadata": json.dumps(metadata),
            }

            # Try to add newer fields if they exist in the model
            # We'll use a try/except approach for each field to handle missing columns
            try:
                # Check if a test persona with this field can be created
                test_persona = Persona(archetype="test")
                # If no exception, add the field to our dictionary
                persona_fields["archetype"] = archetype
            except Exception:
                logger.info("'archetype' column not found in Persona model, skipping")

            # Try adding other new fields using the same approach
            field_mappings = {
                "demographics": demographics,
                "goals_and_motivations": goals,
                "skills_and_expertise": skills,
                "workflow_and_environment": workflow,
                "challenges_and_frustrations": pain_points_value,
                "needs_and_desires": needs,
                "technology_and_tools": tools,
                "attitude_towards_research": analysis_approach_value,
                "attitude_towards_ai": ai_attitude,
                "key_quotes": quotes,
                "overall_confidence": confidence,
                "supporting_evidence_summary": evidence,
            }

            # Try each field and add it if it exists
            for field_name, field_value in field_mappings.items():
                try:
                    # Create a test persona with just this field
                    test_kwargs = {
                        field_name: (
                            field_value
                            if isinstance(field_value, (int, float))
                            else json.dumps(field_value)
                        )
                    }
                    test_persona = Persona(**test_kwargs)
                    # If no exception, add the field to our dictionary
                    persona_fields[field_name] = (
                        field_value
                        if isinstance(field_value, (int, float))
                        else json.dumps(field_value)
                    )
                except Exception:
                    logger.info(
                        f"'{field_name}' column not found in Persona model, skipping"
                    )

            # Create the Persona object with the fields we've determined exist
            new_persona = Persona(**persona_fields)
        except TypeError as e:
            # If we get a TypeError, it's likely because we're trying to use a field that doesn't exist
            # Log the error and try a more conservative approach
            logger.error(f"Error creating Persona with all fields: {str(e)}")

            # Try with only the fields we know exist in all versions of the model
            # Create a dictionary of fields to initialize the Persona object
            persona_fields = {
                "result_id": result_id,
                "name": name,
                "description": description,
                # Store only essential fields that we know exist
                "role_context": json.dumps(demographics),
                "key_responsibilities": json.dumps(goals),
                "tools_used": json.dumps(tools),
                "collaboration_style": json.dumps(collab_style_value),
                "analysis_approach": json.dumps(analysis_approach_value),
                "pain_points": json.dumps(pain_points_value),
                "patterns": json.dumps(patterns),
                "confidence": confidence,  # Use the float directly
                "evidence": json.dumps(evidence),
                "persona_metadata": json.dumps(metadata),
            }

            # Create the Persona object with the fields we know exist
            new_persona = Persona(**persona_fields)

        # Add to database
        try:
            self.db.add(new_persona)
            self.db.commit()
            logger.info(
                f"Persona '{name}' saved to database for result_id: {result_id}"
            )
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error saving persona '{name}' to database: {str(e)}")

    def _extract_sentiment_statements_from_data(
        self, themes, patterns
    ) -> Dict[str, List[str]]:
        """
        Extract sentiment statements from themes and patterns data.
        This is a fallback when the LLM doesn't directly generate sentiment statements.

        Args:
            themes: List of theme objects
            patterns: List of pattern objects

        Returns:
            Dictionary with lists of positive, neutral, and negative statements
        """
        sentiment_statements = {"positive": [], "neutral": [], "negative": []}

        # Process themes based on their sentiment scores
        for theme in themes:
            # Skip themes without statements or sentiment
            if not theme.get("statements") or "sentiment" not in theme:
                continue

            sentiment_score = theme.get("sentiment", 0)
            statements = theme.get("statements", []) or theme.get("examples", [])

            # Use all statements from each theme, not just 2
            for statement in statements:
                # Only add unique statements
                if (
                    sentiment_score > 0.3
                    and statement not in sentiment_statements["positive"]
                ):
                    sentiment_statements["positive"].append(statement)
                elif (
                    sentiment_score < -0.3
                    and statement not in sentiment_statements["negative"]
                ):
                    sentiment_statements["negative"].append(statement)
                elif statement not in sentiment_statements["neutral"]:
                    sentiment_statements["neutral"].append(statement)

        # Process patterns to supplement the statements
        for pattern in patterns:
            # Skip patterns without examples or sentiment
            if not pattern.get("examples") and not pattern.get("evidence"):
                continue

            sentiment_score = pattern.get("sentiment", 0)
            statements = pattern.get("examples", []) or pattern.get("evidence", [])

            # Use all statements from each pattern, not just 1
            for statement in statements:
                # Only add unique statements
                if (
                    sentiment_score > 0.3
                    and statement not in sentiment_statements["positive"]
                ):
                    sentiment_statements["positive"].append(statement)
                elif (
                    sentiment_score < -0.3
                    and statement not in sentiment_statements["negative"]
                ):
                    sentiment_statements["negative"].append(statement)
                elif statement not in sentiment_statements["neutral"]:
                    sentiment_statements["neutral"].append(statement)

        # Limit to 15 statements per category (instead of 5)
        sentiment_statements["positive"] = sentiment_statements["positive"][:15]
        sentiment_statements["neutral"] = sentiment_statements["neutral"][:15]
        sentiment_statements["negative"] = sentiment_statements["negative"][:15]

        return sentiment_statements

"""
Persona builder module for constructing personas from attributes.

This module provides functionality for:
1. Building persona objects from attributes
2. Creating fallback personas
3. Validating personas
"""

from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field, asdict
import logging
import re
import json
from datetime import datetime
from backend.services.processing.demographic_extractor import DemographicExtractor
from backend.services.processing.pattern_categorizer import PatternCategorizer
from pydantic import ValidationError

# Import Pydantic schema for validation
try:
    from schemas import Persona as PersonaSchema
except ImportError:
    try:
        from schemas import Persona as PersonaSchema
    except ImportError:
        logger = logging.getLogger(__name__)
        logger.warning("Could not import PersonaSchema, validation will be limited")
        # We'll define a minimal schema later if needed

# Configure logging
logger = logging.getLogger(__name__)


@dataclass
class PersonaTrait:
    """A trait of a persona with evidence and confidence"""
    value: Union[str, dict, list]
    confidence: float
    evidence: List[str] = field(default_factory=list)


@dataclass
class Persona:
    """A user persona derived from interview data"""
    name: str
    description: str
    role_context: PersonaTrait
    key_responsibilities: PersonaTrait
    tools_used: PersonaTrait
    collaboration_style: PersonaTrait
    analysis_approach: PersonaTrait
    pain_points: PersonaTrait
    patterns: List[str] = field(default_factory=list)
    confidence: float = 0.7  # Default confidence
    evidence: List[str] = field(default_factory=list)
    persona_metadata: Optional[Dict[str, Any]] = None  # Changed from metadata
    role_in_interview: str = "Participant"  # Default role in the interview (Interviewee, Interviewer, etc.)

    # New fields
    archetype: str = "Unknown"
    demographics: Optional[PersonaTrait] = None
    goals_and_motivations: Optional[PersonaTrait] = None
    skills_and_expertise: Optional[PersonaTrait] = None
    workflow_and_environment: Optional[PersonaTrait] = None
    challenges_and_frustrations: Optional[PersonaTrait] = None
    needs_and_desires: Optional[PersonaTrait] = None
    technology_and_tools: Optional[PersonaTrait] = None
    attitude_towards_research: Optional[PersonaTrait] = None
    attitude_towards_ai: Optional[PersonaTrait] = None
    key_quotes: Optional[PersonaTrait] = None


def persona_to_dict(persona: Persona) -> Dict[str, Any]:
    """
    Convert a Persona object to a dictionary.

    Args:
        persona: Persona object

    Returns:
        Dictionary representation of the persona
    """
    # Check if persona is None
    if persona is None:
        logger = logging.getLogger(__name__)
        logger.warning("Received None persona in persona_to_dict, returning empty dict")
        return {
            "name": "Unknown",
            "description": "No persona data available",
            "confidence": 0.0,
            "evidence": ["No data available"],
            "metadata": {"error": "No persona data available"}
        }

    # Convert to dictionary using dataclasses.asdict
    persona_dict = asdict(persona)

    # Rename metadata field for backward compatibility
    if "persona_metadata" in persona_dict:
        persona_dict["metadata"] = persona_dict.pop("persona_metadata")

    # Add aliases for backward compatibility
    persona_dict["overall_confidence"] = persona_dict["confidence"]
    persona_dict["supporting_evidence_summary"] = persona_dict["evidence"]

    return persona_dict


class PersonaBuilder:
    """
    Builds persona objects from attributes.
    """

    def __init__(self):
        """Initialize the persona builder."""
        logger.info("Initialized PersonaBuilder")
        self.demographic_extractor = DemographicExtractor()
        self.pattern_categorizer = PatternCategorizer()

    def build_persona_from_attributes(
        self, attributes: Dict[str, Any], name_override: str = "", role: str = "Participant"
    ) -> Persona:
        """
        Build a persona object from attributes.

        Args:
            attributes: Persona attributes
            name_override: Optional name to override the one in attributes
            role: Role of the person in the interview

        Returns:
            Persona object
        """
        logger.info(f"Building persona from attributes for {role}")

        try:
            # Extract name from attributes or use override
            name = name_override
            if not name:
                # Get name from attributes, ensuring it's a string
                attr_name = attributes.get("name", role)
                if isinstance(attr_name, str):
                    name = attr_name
                else:
                    # If name is not a string, use role as fallback
                    logger.warning(f"Name is not a string: {attr_name}, using role as fallback")
                    name = role

            # Log the attributes for debugging
            logger.info(f"Building persona with name: {name}, role: {role}")
            logger.info(f"Attribute keys: {list(attributes.keys())}")

            # Get overall confidence score if available (for simplified format)
            overall_confidence = 0.7  # Default confidence
            if "overall_confidence_score" in attributes and isinstance(attributes["overall_confidence_score"], (int, float)):
                overall_confidence = float(attributes["overall_confidence_score"])
            elif "overall_confidence" in attributes and isinstance(attributes["overall_confidence"], (int, float)):
                overall_confidence = float(attributes["overall_confidence"])

            # Handle key_quotes specially - it can be a string, list, or dict
            key_quotes_data = attributes.get("key_quotes", {})
            key_quotes_list = []

            if isinstance(key_quotes_data, list):
                # For simplified format: list of quotes
                key_quotes_list = key_quotes_data
                # Convert list of quotes to a trait
                key_quotes_value = ", ".join(key_quotes_data[:5])  # Take first 5 quotes
                key_quotes_data = {
                    "value": key_quotes_value,
                    "confidence": overall_confidence,  # Use overall confidence
                    "evidence": key_quotes_data  # Use the quotes as evidence
                }
                logger.info(f"Converted key_quotes list to trait: {len(key_quotes_list)} quotes")
            elif isinstance(key_quotes_data, str):
                # Convert string to a trait
                key_quotes_list = [key_quotes_data]
                key_quotes_data = {
                    "value": key_quotes_data,
                    "confidence": overall_confidence,  # Use overall confidence
                    "evidence": [key_quotes_data]
                }
                logger.info(f"Converted key_quotes string to trait")
            elif isinstance(key_quotes_data, dict) and "value" in key_quotes_data:
                # Already in trait format
                if "evidence" in key_quotes_data and isinstance(key_quotes_data["evidence"], list):
                    key_quotes_list = key_quotes_data["evidence"]
                # Ensure confidence uses overall_confidence if not specified
                if "confidence" not in key_quotes_data:
                    key_quotes_data["confidence"] = overall_confidence
            else:
                # Default empty trait
                key_quotes_data = {
                    "value": "",
                    "confidence": overall_confidence,
                    "evidence": []
                }
                logger.info(f"Created default key_quotes trait")

            # Process trait fields with robust error handling
            trait_fields = {
                "role_context": attributes.get("role_context", {}),
                "key_responsibilities": attributes.get("key_responsibilities", {}),
                "tools_used": attributes.get("tools_used", {}),
                "collaboration_style": attributes.get("collaboration_style", {}),
                "analysis_approach": attributes.get("analysis_approach", {}),
                "pain_points": attributes.get("pain_points", {}),
                "demographics": attributes.get("demographics", {}),
                "goals_and_motivations": attributes.get("goals_and_motivations", {}),
                "skills_and_expertise": attributes.get("skills_and_expertise", {}),
                "workflow_and_environment": attributes.get("workflow_and_environment", {}),
                "challenges_and_frustrations": attributes.get("challenges_and_frustrations", {}),
                "needs_and_desires": attributes.get("needs_and_desires", {}),
                "technology_and_tools": attributes.get("technology_and_tools", {}),
                "attitude_towards_research": attributes.get("attitude_towards_research", {}),
                "attitude_towards_ai": attributes.get("attitude_towards_ai", {})
            }

            # Process each trait field to ensure proper format
            processed_traits = {}
            for field_name, field_data in trait_fields.items():
                if isinstance(field_data, dict) and "value" in field_data:
                    # Standard format - already a dict with value, confidence, evidence
                    processed_trait = field_data.copy()
                    # Ensure confidence uses overall_confidence if not specified
                    if "confidence" not in processed_trait:
                        processed_trait["confidence"] = overall_confidence
                    # Ensure evidence is a list
                    if "evidence" not in processed_trait or not isinstance(processed_trait["evidence"], list):
                        processed_trait["evidence"] = key_quotes_list[:2] if key_quotes_list else []

                    # Special processing for demographics field
                    if field_name == "demographics":
                        # Get all evidence from other fields for context
                        all_evidence = []
                        for other_field, other_data in processed_traits.items():
                            if isinstance(other_data, dict) and "evidence" in other_data:
                                all_evidence.extend(other_data.get("evidence", []))

                        # Use the demographic extractor to enhance the demographics field
                        processed_trait = self.demographic_extractor.extract_demographics(processed_trait, all_evidence)

                    processed_traits[field_name] = processed_trait
                    logger.debug(f"Field {field_name} already in trait format")
                elif isinstance(field_data, str) and field_data.strip():
                    # String format - convert to dict (simplified format)
                    processed_traits[field_name] = {
                        "value": field_data,
                        "confidence": overall_confidence,
                        "evidence": key_quotes_list[:2] if key_quotes_list else []  # Use key quotes as evidence
                    }
                    logger.debug(f"Converted string field {field_name} to trait")
                elif isinstance(field_data, list) and field_data:
                    # List format - convert to dict
                    processed_traits[field_name] = {
                        "value": ", ".join(str(item) for item in field_data[:3]),  # Take first 3 items
                        "confidence": overall_confidence,
                        "evidence": key_quotes_list[:2] if key_quotes_list else []  # Use key quotes as evidence
                    }
                    logger.debug(f"Converted list field {field_name} to trait")
                else:
                    # Default empty dict
                    processed_traits[field_name] = {
                        "value": "",
                        "confidence": overall_confidence,  # Use overall confidence
                        "evidence": []
                    }
                    logger.debug(f"Created default trait for field {field_name}")

            # Add key_quotes to processed traits
            processed_traits["key_quotes"] = key_quotes_data

            # If key_quotes is empty, populate it with quotes from other fields
            if not key_quotes_data.get("evidence") or not key_quotes_data.get("value"):
                logger.info("Key quotes are empty, collecting quotes from other fields")
                all_quotes = []

                # Collect evidence from all other fields
                for field_name, field_data in processed_traits.items():
                    if field_name != "key_quotes" and isinstance(field_data, dict) and "evidence" in field_data:
                        if isinstance(field_data["evidence"], list):
                            all_quotes.extend(field_data["evidence"])

                # Remove duplicates and limit to 5-7 quotes
                unique_quotes = []
                for quote in all_quotes:
                    if quote and isinstance(quote, str) and quote not in unique_quotes:
                        unique_quotes.append(quote)
                        if len(unique_quotes) >= 7:
                            break

                if unique_quotes:
                    logger.info(f"Collected {len(unique_quotes)} quotes from other fields")
                    # Format the value field to be a descriptive summary
                    value_summary = "Key representative quotes that capture the persona's authentic voice and perspective"
                    processed_traits["key_quotes"] = {
                        "value": value_summary,
                        "confidence": overall_confidence,
                        "evidence": unique_quotes[:7]  # Limit to 7 quotes
                    }

            # Extract patterns and evidence
            patterns = attributes.get("patterns", [])
            if isinstance(patterns, str):
                # Split string by commas or newlines
                patterns = [p.strip() for p in re.split(r'[,\n]', patterns) if p.strip()]
            elif not isinstance(patterns, list):
                patterns = []

            # Enhance patterns with more structure if possible
            structured_patterns = []
            if patterns:
                # Try to extract patterns from other fields if patterns list is too short
                if len(patterns) < 3:
                    # Look for patterns in other fields' evidence
                    pattern_keywords = ["always", "often", "tends to", "prefers", "values", "struggles with", "focuses on"]
                    for field_name, field_data in processed_traits.items():
                        if isinstance(field_data, dict) and "evidence" in field_data:
                            for evidence in field_data.get("evidence", []):
                                for keyword in pattern_keywords:
                                    if keyword in evidence.lower():
                                        potential_pattern = evidence.strip()
                                        if potential_pattern and potential_pattern not in patterns:
                                            patterns.append(potential_pattern)
                                            break

                # Format patterns with more structure
                for pattern in patterns:
                    # Clean up the pattern
                    clean_pattern = pattern.strip()
                    if not clean_pattern:
                        continue

                    # Add to structured patterns
                    structured_patterns.append(clean_pattern)

                    # Limit to 7 patterns
                    if len(structured_patterns) >= 7:
                        break

            # Use structured patterns if available, otherwise use original patterns
            patterns = structured_patterns if structured_patterns else patterns

            # Categorize patterns and make them more actionable
            if patterns:
                logger.info(f"Categorizing {len(patterns)} patterns")

                # First categorize the patterns
                categorized_patterns = self.pattern_categorizer.categorize_patterns(patterns)

                # Store the original patterns for reference
                original_patterns = patterns.copy()

                # Format patterns as JSON for structured data
                json_patterns = self.pattern_categorizer.format_patterns_as_json(categorized_patterns)

                # Then format them for display
                formatted_patterns = self.pattern_categorizer.format_patterns_for_display(categorized_patterns)

                # Use the formatted patterns
                patterns = formatted_patterns

                logger.info(f"Categorized {len(patterns)} patterns into {len(set([p.split(':')[0].split('(')[0].strip() for p in patterns]))} categories")

                # Store the JSON patterns in the attributes for later use
                attributes["patterns_json"] = json_patterns

            evidence = attributes.get("evidence", [])
            if isinstance(evidence, str):
                # Split string by commas or newlines
                evidence = [e.strip() for e in re.split(r'[,\n]', evidence) if e.strip()]
            elif not isinstance(evidence, list):
                evidence = []

            # Extract confidence
            confidence = attributes.get("confidence", 0.7)
            if not isinstance(confidence, (int, float)):
                try:
                    confidence = float(confidence)
                except (ValueError, TypeError):
                    confidence = 0.7

            # Ensure confidence is between 0 and 1
            confidence = max(0.0, min(1.0, confidence))

            # Create Persona object
            persona = Persona(
                name=name,
                description=self._get_string_value(attributes.get("description"), "No description provided."),
                archetype=self._get_string_value(attributes.get("archetype"), "Unknown"),

                # Create PersonaTrait instances from processed data
                role_context=PersonaTrait(
                    value=processed_traits["role_context"].get("value", ""),
                    confidence=float(processed_traits["role_context"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["role_context"].get("evidence", [])),
                ),
                key_responsibilities=PersonaTrait(
                    value=processed_traits["key_responsibilities"].get("value", ""),
                    confidence=float(processed_traits["key_responsibilities"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["key_responsibilities"].get("evidence", [])),
                ),
                tools_used=PersonaTrait(
                    value=processed_traits["tools_used"].get("value", ""),
                    confidence=float(processed_traits["tools_used"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["tools_used"].get("evidence", [])),
                ),
                collaboration_style=PersonaTrait(
                    value=processed_traits["collaboration_style"].get("value", ""),
                    confidence=float(processed_traits["collaboration_style"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["collaboration_style"].get("evidence", [])),
                ),
                analysis_approach=PersonaTrait(
                    value=processed_traits["analysis_approach"].get("value", ""),
                    confidence=float(processed_traits["analysis_approach"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["analysis_approach"].get("evidence", [])),
                ),
                pain_points=PersonaTrait(
                    value=processed_traits["pain_points"].get("value", ""),
                    confidence=float(processed_traits["pain_points"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["pain_points"].get("evidence", [])),
                ),

                # Create PersonaTrait instances for new fields
                demographics=PersonaTrait(
                    value=processed_traits["demographics"].get("value", ""),
                    confidence=float(processed_traits["demographics"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["demographics"].get("evidence", [])),
                ),
                goals_and_motivations=PersonaTrait(
                    value=processed_traits["goals_and_motivations"].get("value", ""),
                    confidence=float(processed_traits["goals_and_motivations"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["goals_and_motivations"].get("evidence", [])),
                ),
                skills_and_expertise=PersonaTrait(
                    value=processed_traits["skills_and_expertise"].get("value", ""),
                    confidence=float(processed_traits["skills_and_expertise"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["skills_and_expertise"].get("evidence", [])),
                ),
                workflow_and_environment=PersonaTrait(
                    value=processed_traits["workflow_and_environment"].get("value", ""),
                    confidence=float(processed_traits["workflow_and_environment"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["workflow_and_environment"].get("evidence", [])),
                ),
                challenges_and_frustrations=PersonaTrait(
                    value=processed_traits["challenges_and_frustrations"].get("value", ""),
                    confidence=float(processed_traits["challenges_and_frustrations"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["challenges_and_frustrations"].get("evidence", [])),
                ),
                needs_and_desires=PersonaTrait(
                    value=processed_traits["needs_and_desires"].get("value", ""),
                    confidence=float(processed_traits["needs_and_desires"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["needs_and_desires"].get("evidence", [])),
                ),
                technology_and_tools=PersonaTrait(
                    value=processed_traits["technology_and_tools"].get("value", ""),
                    confidence=float(processed_traits["technology_and_tools"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["technology_and_tools"].get("evidence", [])),
                ),
                attitude_towards_research=PersonaTrait(
                    value=processed_traits["attitude_towards_research"].get("value", ""),
                    confidence=float(processed_traits["attitude_towards_research"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["attitude_towards_research"].get("evidence", [])),
                ),
                attitude_towards_ai=PersonaTrait(
                    value=processed_traits["attitude_towards_ai"].get("value", ""),
                    confidence=float(processed_traits["attitude_towards_ai"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["attitude_towards_ai"].get("evidence", [])),
                ),
                key_quotes=PersonaTrait(
                    value=processed_traits["key_quotes"].get("value", ""),
                    confidence=float(processed_traits["key_quotes"].get("confidence", 0.7)),
                    evidence=self._clean_evidence_list(processed_traits["key_quotes"].get("evidence", [])),
                ),

                # Set other fields
                patterns=patterns,
                confidence=confidence,
                evidence=evidence,
                persona_metadata=self._create_metadata(attributes, role),
                role_in_interview=role
            )

            # For simplified format, use the overall_confidence_score directly if available
            if "overall_confidence_score" in attributes and isinstance(attributes["overall_confidence_score"], (int, float)):
                persona.confidence = float(attributes["overall_confidence_score"])
                logger.info(f"Using overall_confidence_score directly: {persona.confidence}")
            else:
                # Calculate overall confidence based on trait confidences
                trait_confidences = [
                    persona.role_context.confidence,
                    persona.key_responsibilities.confidence,
                    persona.tools_used.confidence,
                    persona.collaboration_style.confidence,
                    persona.analysis_approach.confidence,
                    persona.pain_points.confidence,
                    persona.demographics.confidence,
                    persona.goals_and_motivations.confidence,
                    persona.skills_and_expertise.confidence,
                    persona.workflow_and_environment.confidence,
                    persona.challenges_and_frustrations.confidence,
                    persona.needs_and_desires.confidence,
                    persona.technology_and_tools.confidence,
                    persona.attitude_towards_research.confidence,
                    persona.attitude_towards_ai.confidence,
                    persona.key_quotes.confidence
                ]

                # Filter out zero confidences
                valid_confidences = [c for c in trait_confidences if c > 0]
                if valid_confidences:
                    persona.confidence = sum(valid_confidences) / len(valid_confidences)
                    logger.info(f"Calculated average confidence from traits: {persona.confidence}")

            # Fix key_quotes field if it's improperly formatted
            if hasattr(persona, "key_quotes") and persona.key_quotes:
                logger.info("Fixing key_quotes field in persona")

                # Extract current key_quotes data
                key_quotes_value = ""
                key_quotes_confidence = 0.7
                key_quotes_evidence = []

                if hasattr(persona.key_quotes, "value"):
                    key_quotes_value = persona.key_quotes.value
                if hasattr(persona.key_quotes, "confidence"):
                    key_quotes_confidence = persona.key_quotes.confidence
                if hasattr(persona.key_quotes, "evidence"):
                    key_quotes_evidence = persona.key_quotes.evidence

                # Fix key_quotes data
                key_quotes_data = self._fix_key_quotes({
                    "value": key_quotes_value,
                    "confidence": key_quotes_confidence,
                    "evidence": key_quotes_evidence
                })

                logger.info(f"Fixed key_quotes: value={key_quotes_data['value'][:50]}..., evidence count={len(key_quotes_data['evidence'])}")

                # Update the key_quotes field
                if hasattr(persona, "key_quotes"):
                    if hasattr(persona.key_quotes, "value"):
                        persona.key_quotes.value = key_quotes_data["value"]
                    if hasattr(persona.key_quotes, "confidence"):
                        persona.key_quotes.confidence = key_quotes_data["confidence"]
                    if hasattr(persona.key_quotes, "evidence"):
                        persona.key_quotes.evidence = key_quotes_data["evidence"]

                    logger.info(f"Updated key_quotes field in persona: evidence count={len(persona.key_quotes.evidence) if hasattr(persona.key_quotes, 'evidence') else 0}")

            # Enhance demographics with role context information
            if hasattr(persona, "demographics") and hasattr(persona, "role_context"):
                logger.info("Enhancing demographics with role context information")

                # Extract current demographics and role_context data
                demographics_data = {
                    "value": persona.demographics.value if hasattr(persona.demographics, "value") else "",
                    "confidence": persona.demographics.confidence if hasattr(persona.demographics, "confidence") else 0.7,
                    "evidence": persona.demographics.evidence if hasattr(persona.demographics, "evidence") else []
                }

                role_context_data = {
                    "value": persona.role_context.value if hasattr(persona.role_context, "value") else "",
                    "confidence": persona.role_context.confidence if hasattr(persona.role_context, "confidence") else 0.7,
                    "evidence": persona.role_context.evidence if hasattr(persona.role_context, "evidence") else []
                }

                # Enhance demographics with role context
                enhanced_demographics = self._enhance_demographics_with_role_context(demographics_data, role_context_data)

                logger.info(f"Enhanced demographics: value={enhanced_demographics['value'][:50]}..., evidence count={len(enhanced_demographics['evidence'])}")

                # Update the demographics field
                if hasattr(persona, "demographics"):
                    if hasattr(persona.demographics, "value"):
                        persona.demographics.value = enhanced_demographics["value"]
                    if hasattr(persona.demographics, "confidence"):
                        persona.demographics.confidence = enhanced_demographics["confidence"]
                    if hasattr(persona.demographics, "evidence"):
                        persona.demographics.evidence = enhanced_demographics["evidence"]

                    logger.info(f"Updated demographics field in persona: evidence count={len(persona.demographics.evidence) if hasattr(persona.demographics, 'evidence') else 0}")

            # Ensure we have at least some evidence
            if not persona.evidence or len(persona.evidence) < 3 or all(e.startswith("Fallback") for e in persona.evidence):
                logger.info("Enhancing evidence field in persona")

                # Collect evidence from traits
                all_evidence = []
                evidence_sources = {}  # Track which field each evidence came from

                # Prioritize certain fields for evidence collection
                priority_fields = [
                    "key_quotes",
                    "pain_points",
                    "skills_and_expertise",
                    "goals_and_motivations",
                    "challenges_and_frustrations",
                    "workflow_and_environment",
                    "tools_used",
                    "collaboration_style",
                    "analysis_approach"
                ]

                # First collect from priority fields
                for field_name in priority_fields:
                    if field_name in processed_traits:
                        trait = getattr(persona, field_name)
                        if trait and trait.evidence:
                            # Get the most representative evidence from this trait
                            best_evidence = sorted(trait.evidence, key=len, reverse=True)[:2]  # Take up to 2 longest pieces of evidence
                            for evidence in best_evidence:
                                all_evidence.append(evidence)
                                evidence_sources[evidence] = field_name

                            logger.info(f"Collected {len(best_evidence)} evidence items from {field_name}")

                # Then collect from other fields if needed
                if len(all_evidence) < 5:
                    for field_name in processed_traits:
                        if field_name not in priority_fields:
                            trait = getattr(persona, field_name)
                            if trait and trait.evidence:
                                # Get one piece of evidence from each non-priority field
                                best_evidence = sorted(trait.evidence, key=len, reverse=True)[:1]
                                for evidence in best_evidence:
                                    all_evidence.append(evidence)
                                    evidence_sources[evidence] = field_name

                                logger.info(f"Collected {len(best_evidence)} evidence items from {field_name}")

                                # Stop if we have enough evidence
                                if len(all_evidence) >= 10:
                                    break

                # Remove duplicates while preserving order
                unique_evidence = []
                for e in all_evidence:
                    if e and e not in unique_evidence and len(e) > 20:  # Only include substantial evidence
                        unique_evidence.append(e)

                logger.info(f"Collected {len(unique_evidence)} unique evidence items")

                # Use the collected evidence
                if unique_evidence:
                    # Limit to 10 pieces of evidence
                    selected_evidence = unique_evidence[:10]

                    # Add descriptive labels to evidence for better context
                    labeled_evidence = []

                    # Define descriptive labels for each field
                    field_labels = {
                        "key_quotes": "Key Quote",
                        "pain_points": "Pain Point",
                        "skills_and_expertise": "Skill/Expertise",
                        "goals_and_motivations": "Goal/Motivation",
                        "challenges_and_frustrations": "Challenge/Frustration",
                        "workflow_and_environment": "Workflow/Environment",
                        "tools_used": "Tool Usage",
                        "collaboration_style": "Collaboration Style",
                        "analysis_approach": "Analysis Approach",
                        "demographics": "Background",
                        "role_context": "Role Context",
                        "key_responsibilities": "Responsibility",
                        "needs_and_desires": "Need/Desire",
                        "technology_and_tools": "Technology/Tool",
                        "attitude_towards_research": "Research Attitude",
                        "attitude_towards_ai": "AI Attitude"
                    }

                    # Add labels to evidence
                    for evidence in selected_evidence:
                        field_name = evidence_sources.get(evidence, "general")
                        label = field_labels.get(field_name, field_name.replace("_", " ").title())

                        # Add a more specific label based on content if possible
                        specific_label = self._get_specific_evidence_label(evidence, label)

                        labeled_evidence.append(f"{specific_label}: {evidence}")

                    logger.info(f"Created {len(labeled_evidence)} labeled evidence items")

                    persona.evidence = labeled_evidence

            # Validate persona with Pydantic if available
            try:
                if 'PersonaSchema' in globals():
                    persona_dict = persona_to_dict(persona)
                    validated_persona = PersonaSchema(**persona_dict)
                    logger.info(f"Successfully validated persona: {persona.name}")
            except ValidationError as e:
                logger.warning(f"Persona validation failed: {str(e)}")

            # Log evidence counts for debugging
            evidence_counts = {}
            for field_name in processed_traits:
                trait = getattr(persona, field_name)
                if trait and hasattr(trait, 'evidence') and trait.evidence:
                    evidence_counts[field_name] = len(trait.evidence)

            logger.info(f"Evidence counts for persona {persona.name}: {evidence_counts}")
            logger.info(f"Successfully built persona: {persona.name}")

            # Ensure evidence is preserved in the persona_to_dict conversion
            test_dict = persona_to_dict(persona)
            for field_name, count in evidence_counts.items():
                if field_name in test_dict and isinstance(test_dict[field_name], dict) and 'evidence' in test_dict[field_name]:
                    actual_count = len(test_dict[field_name]['evidence'])
                    if actual_count != count:
                        logger.warning(f"Evidence count mismatch for {field_name}: {count} in persona object, {actual_count} in dict")

            return persona

        except Exception as e:
            logger.error(f"Error building persona from attributes: {str(e)}", exc_info=True)
            return self.create_fallback_persona(role, name_override)

    def create_fallback_persona(self, role: str = "Participant", name: str = "") -> Persona:
        """
        Create a fallback persona when building fails.

        Args:
            role: Role of the person in the interview
            name: Optional name for the persona (defaults to "Default {role}")

        Returns:
            Fallback persona
        """
        logger.info(f"Creating fallback persona for {role}")

        # Create a minimal trait
        minimal_trait = PersonaTrait(
            value="Unknown",
            confidence=0.3,
            evidence=["Fallback due to processing error"]
        )

        # Use provided name or generate default name
        persona_name = name if name else f"Default {role}"

        # Create fallback persona
        return Persona(
            name=persona_name,
            description=f"Default {role} due to processing error",
            archetype="Unknown",
            # Legacy fields
            role_context=minimal_trait,
            key_responsibilities=minimal_trait,
            tools_used=minimal_trait,
            collaboration_style=minimal_trait,
            analysis_approach=minimal_trait,
            pain_points=minimal_trait,
            # New fields
            demographics=minimal_trait,
            goals_and_motivations=minimal_trait,
            skills_and_expertise=minimal_trait,
            workflow_and_environment=minimal_trait,
            challenges_and_frustrations=minimal_trait,
            needs_and_desires=minimal_trait,
            technology_and_tools=minimal_trait,
            attitude_towards_research=minimal_trait,
            attitude_towards_ai=minimal_trait,
            key_quotes=PersonaTrait(
                value="No quotes available",
                confidence=0.3,
                evidence=["Fallback due to processing error"]
            ),
            # Other fields
            patterns=[],
            confidence=0.3,
            evidence=["Fallback due to processing error"],
            persona_metadata={
                "source": "fallback_persona",
                "timestamp": datetime.now().isoformat(),
                "role": role,
                "is_fallback": True,
                "overall_confidence_score": 0.3
            },
            role_in_interview=role
        )

    def _clean_evidence_list(self, evidence: Any) -> List[str]:
        """
        Clean and validate evidence list.

        Args:
            evidence: Evidence data

        Returns:
            Cleaned evidence list
        """
        if not evidence:
            return []

        if isinstance(evidence, str):
            # Check if the string looks like a JSON array
            if evidence.startswith('[') and evidence.endswith(']'):
                try:
                    # Try to parse it as JSON
                    import json
                    parsed_evidence = json.loads(evidence.replace("'", '"'))
                    if isinstance(parsed_evidence, list):
                        return [str(e) for e in parsed_evidence if e]
                except Exception as e:
                    logger.warning(f"Failed to parse evidence string as JSON: {e}")
            return [evidence]

        if isinstance(evidence, list):
            return [str(e) for e in evidence if e]

        return []

    def _fix_key_quotes(self, key_quotes_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Fix key quotes data that might be improperly formatted.

        Args:
            key_quotes_data: Key quotes data

        Returns:
            Fixed key quotes data
        """
        if not key_quotes_data:
            return {"value": "", "confidence": 0.7, "evidence": []}

        # Check if value is a JSON string
        value = key_quotes_data.get("value", "")
        evidence = key_quotes_data.get("evidence", [])
        confidence = key_quotes_data.get("confidence", 0.7)

        logger.info(f"Fixing key_quotes: value type={type(value)}, value={value[:100]}...")

        # Handle JSON string in value field
        if isinstance(value, str):
            # Check if it looks like a JSON array
            if (value.startswith('[') and value.endswith(']')) or (value.startswith('{') and value.endswith('}')):
                try:
                    # Try to parse it as JSON
                    import json
                    # Replace single quotes with double quotes for JSON parsing
                    # Also handle escaped quotes
                    json_value = value.replace("'", '"').replace('\\"', '"')
                    parsed_value = json.loads(json_value)

                    logger.info(f"Successfully parsed key_quotes JSON: {type(parsed_value)}")

                    if isinstance(parsed_value, list) and parsed_value:
                        # If evidence is empty but we have quotes in value, move them to evidence
                        if not evidence or len(evidence) == 0:
                            evidence = parsed_value
                            logger.info(f"Moved {len(evidence)} quotes from value to evidence")
                        # Set value to a descriptive summary
                        value = "Key representative quotes that capture the persona's authentic voice and perspective"
                    elif isinstance(parsed_value, dict) and "value" in parsed_value:
                        # Handle nested structure
                        nested_value = parsed_value.get("value")
                        nested_evidence = parsed_value.get("evidence", [])

                        if isinstance(nested_value, list) and nested_value:
                            evidence = nested_value
                            logger.info(f"Extracted {len(evidence)} quotes from nested value")
                        elif isinstance(nested_evidence, list) and nested_evidence:
                            evidence = nested_evidence
                            logger.info(f"Extracted {len(evidence)} quotes from nested evidence")

                        value = "Key representative quotes that capture the persona's authentic voice and perspective"
                except Exception as e:
                    logger.warning(f"Failed to parse key_quotes value as JSON: {e}")

                    # Fallback: try to extract quotes using regex
                    if not evidence or len(evidence) == 0:
                        import re
                        # Look for quoted strings
                        quotes = re.findall(r'"([^"]*)"', value)
                        if quotes:
                            evidence = quotes
                            logger.info(f"Extracted {len(evidence)} quotes using regex")
                            value = "Key representative quotes that capture the persona's authentic voice and perspective"

        # Ensure evidence is a list of strings
        if evidence and isinstance(evidence, list):
            # Clean up evidence items
            cleaned_evidence = []
            for item in evidence:
                if item and isinstance(item, str):
                    # Remove extra quotes and clean up
                    clean_item = item.strip()
                    if clean_item.startswith('"') and clean_item.endswith('"'):
                        clean_item = clean_item[1:-1]
                    if clean_item:
                        cleaned_evidence.append(clean_item)

            evidence = cleaned_evidence
            logger.info(f"Cleaned up evidence, now have {len(evidence)} quotes")

        # If we still don't have evidence but have a value, use the value as a single quote
        if (not evidence or len(evidence) == 0) and value and value != "Key representative quotes that capture the persona's authentic voice and perspective":
            evidence = [value]
            value = "Key representative quotes that capture the persona's authentic voice and perspective"
            logger.info("Used value as a single quote in evidence")

        return {
            "value": value,
            "confidence": confidence,
            "evidence": evidence
        }

    def _enhance_demographics_with_role_context(self, demographics: Dict[str, Any], role_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhance demographics with information from role context.

        Args:
            demographics: Demographics data
            role_context: Role context data

        Returns:
            Enhanced demographics data
        """
        # Get existing values
        demo_value = demographics.get("value", "")
        role_value = role_context.get("value", "")

        # Combine evidence
        combined_evidence = demographics.get("evidence", []) + role_context.get("evidence", [])
        combined_evidence = list(set(combined_evidence))  # Remove duplicates

        # Extract company and organizational information from role context
        company_info = []
        role_info = []

        # Look for company mentions in role context
        for company_pattern in ["company", "organization", "startup", "firm", "employer", "Volkswagen", "Google", "Microsoft", "Amazon", "Apple"]:
            if company_pattern in role_value:
                # Extract the sentence containing the company
                sentences = role_value.split(". ")
                for sentence in sentences:
                    if company_pattern in sentence:
                        company_info.append(sentence.strip())
                        break

        # Look for role mentions in role context
        for role_pattern in ["role", "position", "job", "title", "responsibility", "designer", "developer", "manager", "director", "lead"]:
            if role_pattern in role_value.lower():
                # Extract the sentence containing the role
                sentences = role_value.split(". ")
                for sentence in sentences:
                    if role_pattern in sentence.lower():
                        role_info.append(sentence.strip())
                        break

        # Create enhanced value
        enhanced_parts = []

        # Start with original demographics
        if demo_value:
            enhanced_parts.append(demo_value)

        # Add company information if not already in demographics
        for info in company_info:
            if info and not any(info.lower() in part.lower() for part in enhanced_parts):
                enhanced_parts.append(info)

        # Add role information if not already in demographics
        for info in role_info:
            if info and not any(info.lower() in part.lower() for part in enhanced_parts):
                enhanced_parts.append(info)

        # Join all parts
        enhanced_value = " | ".join(enhanced_parts)

        return {
            "value": enhanced_value,
            "confidence": max(demographics.get("confidence", 0.7), role_context.get("confidence", 0.7)),
            "evidence": combined_evidence
        }

    def _get_specific_evidence_label(self, evidence: str, default_label: str) -> str:
        """
        Get a more specific label for evidence based on its content.

        Args:
            evidence: Evidence text
            default_label: Default label to use if no specific label can be determined

        Returns:
            Specific label for the evidence
        """
        evidence_lower = evidence.lower()

        # Define patterns for specific labels
        label_patterns = {
            "Career Background": ["background", "education", "degree", "graduated", "university", "college", "school"],
            "Experience": ["experience", "years", "worked", "working", "job", "position", "role"],
            "Skill": ["skill", "know how to", "able to", "capable of", "proficient", "expertise"],
            "Tool Preference": ["prefer", "like", "use", "tool", "software", "platform", "application"],
            "Challenge": ["challenge", "difficult", "hard", "struggle", "problem", "issue", "obstacle"],
            "Frustration": ["frustrat", "annoy", "irritat", "bother", "upset", "disappoint"],
            "Goal": ["goal", "aim", "objective", "target", "want to", "trying to", "hope to"],
            "Motivation": ["motivat", "drive", "inspire", "passion", "interest", "excite"],
            "Need": ["need", "require", "must have", "essential", "necessary", "important"],
            "Desire": ["desire", "wish", "want", "would like", "prefer", "hope for"],
            "Workflow": ["workflow", "process", "procedure", "approach", "method", "routine"],
            "Collaboration": ["collaborat", "team", "work with", "together", "colleague", "partner"],
            "Research Approach": ["research", "study", "investigate", "analyze", "examine", "explore"],
            "Pain Point": ["pain", "frustrat", "annoy", "difficult", "challenge", "problem", "hate"],
            "Quote": ["\"", "'", "said", "mentioned", "stated", "expressed"],
            "Company": ["company", "organization", "startup", "firm", "employer", "corporation", "business"],
            "Role": ["designer", "developer", "manager", "director", "lead", "specialist", "analyst", "consultant"]
        }

        # Check for matches
        for label, patterns in label_patterns.items():
            for pattern in patterns:
                if pattern in evidence_lower:
                    return label

        # If no specific label found, use the default
        return default_label

    def _get_string_value(self, value: Any, default: str = "") -> str:
        """
        Get a string value from a value of any type.

        Args:
            value: Value to convert to string
            default: Default value if value is not a string

        Returns:
            String value
        """
        if isinstance(value, str):
            return value
        elif value is None:
            return default
        else:
            try:
                # Try to convert to string
                return str(value)
            except Exception as e:
                logger.warning(f"Could not convert value to string: {e}")
                return default

    def _create_metadata(self, attributes: Dict[str, Any], role: str) -> Dict[str, Any]:
        """
        Create metadata for persona.

        Args:
            attributes: Persona attributes
            role: Role of the person in the interview

        Returns:
            Metadata dictionary
        """
        metadata = {
            "timestamp": datetime.now().isoformat(),
            "source": "attribute_extraction",
            "role": role
        }

        # Include existing metadata if available
        if "metadata" in attributes and isinstance(attributes["metadata"], dict):
            metadata.update(attributes["metadata"])

        # Include persona_metadata if available (for backward compatibility)
        if "persona_metadata" in attributes and isinstance(attributes["persona_metadata"], dict):
            metadata.update(attributes["persona_metadata"])

        return metadata

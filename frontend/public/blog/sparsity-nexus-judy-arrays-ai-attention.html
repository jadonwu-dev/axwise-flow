<!doctype html>
<html lang="en" class="scroll-smooth">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <!-- SEO Meta Tags -->
    <title>
      The Sparsity Nexus: Bridging Data Structures and AI Attention | AxWise
      Research
    </title>
    <meta
      name="description"
      content="Explore how classical data structures like Judy arrays can revolutionize AI attention mechanisms. Discover three innovative hybrid architectures that could solve the quadratic complexity bottleneck in Large Language Models."
    />
    <meta
      name="keywords"
      content="AI attention mechanisms, Judy arrays, sparse data structures, LLM optimization, transformer architecture, GPU computing, attention bottleneck, AI research, machine learning efficiency"
    />
    <meta name="author" content="AxWise Research Team" />
    <meta name="robots" content="index, follow" />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article" />
    <meta
      property="og:title"
      content="The Sparsity Nexus: Bridging Data Structures and AI Attention"
    />
    <meta
      property="og:description"
      content="Explore how classical data structures like Judy arrays can revolutionize AI attention mechanisms. Discover three innovative hybrid architectures for LLM optimization."
    />
    <meta
      property="og:url"
      content="https://axwise.de/blog/sparsity-nexus-judy-arrays-ai-attention"
    />
    <meta property="og:site_name" content="AxWise Blog" />

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image" />
    <meta
      property="twitter:title"
      content="The Sparsity Nexus: Bridging Data Structures and AI Attention"
    />
    <meta
      property="twitter:description"
      content="Explore how classical data structures like Judy arrays can revolutionize AI attention mechanisms. Discover three innovative hybrid architectures for LLM optimization."
    />

    <!-- Canonical URL -->
    <link
      rel="canonical"
      href="https://axwise.de/blog/sparsity-nexus-judy-arrays-ai-attention"
    />

    <!-- Schema.org structured data -->
    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "headline": "The Sparsity Nexus: Bridging Data Structures and AI Attention",
        "description": "Explore how classical data structures like Judy arrays can revolutionize AI attention mechanisms. Discover three innovative hybrid architectures that could solve the quadratic complexity bottleneck in Large Language Models.",
        "author": {
          "@type": "Organization",
          "name": "AxWise",
          "url": "https://axwise.de"
        },
        "publisher": {
          "@type": "Organization",
          "name": "AxWise",
          "url": "https://axwise.de",
          "logo": {
            "@type": "ImageObject",
            "url": "https://axwise.de/logo.png"
          }
        },
        "datePublished": "2025-06-11",
        "dateModified": "2025-06-11",
        "mainEntityOfPage": {
          "@type": "WebPage",
          "@id": "https://axwise.de/blog/sparsity-nexus-judy-arrays-ai-attention"
        },
        "articleSection": "AI Research",
        "keywords": [
          "AI attention mechanisms",
          "Judy arrays",
          "sparse data structures",
          "LLM optimization",
          "transformer architecture"
        ]
      }
    </script>

    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap"
      rel="stylesheet"
    />

    <style>
      body {
        font-family: "Inter", sans-serif;
        background-color: #f8fafc;
        color: #003f5c;
      }
      .chart-container {
        position: relative;
        width: 100%;
        max-width: 800px;
        margin-left: auto;
        margin-right: auto;
        height: 40vh;
        max-height: 450px;
      }
      .card {
        background-color: white;
        border-radius: 1rem;
        box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1),
          0 2px 4px -2px rgb(0 0 0 / 0.1);
        transition: transform 0.3s ease, box-shadow 0.3s ease;
      }
      .card:hover {
        transform: translateY(-5px);
        box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1),
          0 4px 6px -4px rgb(0 0 0 / 0.1);
      }
      .flow-diagram-step {
        position: relative;
        padding-left: 2.5rem;
        padding-bottom: 1.5rem;
        border-left: 2px solid #665191;
      }
      .flow-diagram-step:last-child {
        border-left-color: transparent;
      }
      .flow-diagram-step::before {
        content: attr(data-step);
        position: absolute;
        left: -1rem;
        top: -0.25rem;
        display: flex;
        align-items: center;
        justify-content: center;
        width: 2rem;
        height: 2rem;
        border-radius: 9999px;
        background-color: #665191;
        color: white;
        font-weight: bold;
        border: 4px solid #f8fafc;
      }
      .nav-link {
        transition: color 0.2s;
      }
      .nav-link:hover {
        color: #ffa600;
      }
      .accent-bg {
        background-color: #ffa600;
      }
      .accent-text {
        color: #ffa600;
      }
      .secondary-accent-bg {
        background-color: #665191;
      }
      .secondary-accent-text {
        color: #665191;
      }
    </style>
  </head>
  <body class="antialiased">
    <!-- Breadcrumb Navigation -->
    <nav class="bg-white border-b border-stone-200" aria-label="Breadcrumb">
      <div class="container mx-auto px-4 py-3">
        <ol class="flex items-center space-x-2 text-sm">
          <li>
            <a href="https://axwise.de" class="text-blue-600 hover:text-blue-800"
              >AxWise</a
            >
          </li>
          <li class="text-stone-400">/</li>
          <li>
            <a
              href="https://axwise.de/blog"
              class="text-blue-600 hover:text-blue-800"
              >Blog</a
            >
          </li>
          <li class="text-stone-400">/</li>
          <li class="text-stone-600" aria-current="page">The Sparsity Nexus</li>
        </ol>
      </div>
    </nav>

    <div class="container mx-auto px-4 py-8 max-w-4xl">
      <article>
        <!-- Article Header -->
        <header class="text-center mb-12">
          <h1
            class="text-4xl md:text-5xl font-bold text-stone-900 mb-6 leading-tight"
          >
            The Sparsity Nexus: Bridging Data Structures and AI Attention
          </h1>
          <p
            class="text-xl text-stone-600 mb-8 max-w-3xl mx-auto leading-relaxed"
          >
            As Large Language Models redefine technology, their growth is
            hitting a fundamental wall: the quadratic complexity of attention.
            This research explores a new path forward, bridging the gap between
            cutting-edge AI and the timeless principles of high-performance data
            structures.
          </p>

          <!-- Article Meta -->
          <div
            class="flex flex-wrap justify-center items-center gap-4 text-sm text-stone-500 mb-8"
          >
            <time datetime="2025-06-11">Published: June 11, 2025</time>
            <span>â€¢</span>
            <span>12 min read</span>
            <span>â€¢</span>
            <span>AI Research</span>
          </div>

          <!-- Tags -->
          <div class="flex flex-wrap justify-center gap-2 mb-8">
            <span
              class="bg-blue-100 text-blue-800 text-xs font-medium px-2.5 py-0.5 rounded"
              >#AI Architecture</span
            >
            <span
              class="bg-green-100 text-green-800 text-xs font-medium px-2.5 py-0.5 rounded"
              >#Data Structures</span
            >
            <span
              class="bg-purple-100 text-purple-800 text-xs font-medium px-2.5 py-0.5 rounded"
              >#LLM Optimization</span
            >
            <span
              class="bg-amber-100 text-amber-800 text-xs font-medium px-2.5 py-0.5 rounded"
              >#Research</span
            >
          </div>

          <!-- White Paper Link -->
          <div class="bg-gradient-to-r from-blue-50 to-indigo-50 p-6 rounded-2xl border border-blue-200 mb-8">
            <div class="text-center">
              <h3 class="text-lg font-semibold text-blue-900 mb-3">ðŸ“„ Academic White Paper Available</h3>
              <p class="text-blue-700 mb-4">
                This research is also available as a comprehensive academic white paper with detailed technical analysis,
                mathematical formulations, and extensive references.
              </p>
              <a href="sparsity-nexus-white-paper.html"
                 class="inline-flex items-center px-6 py-3 bg-blue-600 text-white font-medium rounded-lg hover:bg-blue-700 transition-colors">
                ðŸ“– Read Full White Paper
              </a>
            </div>
          </div>
        </header>

        <!-- Sticky Navigation -->
        <nav
          class="sticky top-0 bg-white/80 backdrop-blur-lg z-50 shadow-sm mb-8 rounded-lg"
        >
          <div class="px-6 py-3 flex justify-center">
            <div class="flex space-x-6 text-sm">
              <a href="#problem" class="nav-link">The Problem</a>
              <a href="#concepts" class="nav-link">Core Concepts</a>
              <a href="#hardware" class="nav-link">Hardware</a>
              <a href="#proposals" class="nav-link">Proposals</a>
              <a href="#outlook" class="nav-link">Outlook</a>
            </div>
          </div>
        </nav>

        <!-- Main Content -->
        <main class="prose prose-lg max-w-none">
          <!-- The Bottleneck -->
          <section id="problem" class="mb-16">
            <h2 class="text-3xl font-bold mb-6">The Attention Bottleneck</h2>
            <p class="text-lg text-stone-600 mb-8">
              Standard Transformer attention scales quadratically (O(NÂ²)). As
              the input sequence (N) grows, the computational cost explodes. The
              interactive chart below visualizes this unsustainable growth
              against a more efficient linearithmic (O(N log N)) approach.
            </p>
            <div class="card p-6 md:p-8 mb-8">
              <div class="chart-container">
                <canvas id="scaling-chart"></canvas>
              </div>
              <div class="mt-8 text-center">
                <label for="sequence-slider" class="block font-medium text-[#003f5c]"
                  >Interactive Sequence Length (N):
                  <span id="slider-value" class="font-bold accent-text"
                    >1024</span
                  ></label
                >
                <input
                  id="sequence-slider"
                  type="range"
                  min="1024"
                  max="65536"
                  step="1024"
                  value="1024"
                  class="w-full max-w-md mt-2 accent-bg"
                />
              </div>
            </div>
          </section>

          <!-- Core Concepts -->
          <section id="concepts" class="mb-16">
            <h2 class="text-3xl font-bold mb-6">A Tale of Two Lookups</h2>
            <p class="text-lg text-stone-600 mb-8">
              At the heart of the issue are two fundamentally different
              approaches to finding information: the probabilistic "soft" lookup
              of attention and the deterministic "hard" lookup of classical data
              structures like Judy arrays.
            </p>
            <div class="grid md:grid-cols-2 gap-8 mb-8">
              <!-- Attention Card -->
              <div class="card p-8">
                <h3 class="text-2xl font-bold secondary-accent-text mb-4">
                  Transformer Attention
                </h3>
                <p class="mb-4">
                  Performs a <strong>probabilistic, similarity-based "soft" lookup</strong>.
                  A Query vector is compared against all Key vectors to
                  calculate relevance, resulting in a weighted sum of all Value
                  vectors. It's built for parallelism on GPUs and is
                  differentiable, allowing it to learn.
                </p>
                <div class="text-sm space-y-2 p-4 bg-slate-50 rounded-lg">
                  <p>
                    <strong class="secondary-accent-text">Lookup:</strong>
                    Probabilistic
                  </p>
                  <p>
                    <strong class="secondary-accent-text">Key:</strong> Dense
                    Vector (Learned)
                  </p>
                  <p>
                    <strong class="secondary-accent-text">Sparsity:</strong>
                    Induced (An optimization)
                  </p>
                  <p>
                    <strong class="secondary-accent-text">Goal:</strong>
                    Maximize Throughput
                  </p>
                </div>
              </div>
              <!-- Judy Array Card -->
              <div class="card p-8">
                <h3 class="text-2xl font-bold secondary-accent-text mb-4">
                  Judy Array Principles
                </h3>
                <p class="mb-4">
                  Performs a <strong>deterministic, exact-match lookup</strong>. A discrete
                  key is used to traverse a highly optimized radix trie to find
                  a specific value. It's engineered to minimize latency by being
                  hyper-aware of CPU cache behavior.
                </p>
                <div class="text-sm space-y-2 p-4 bg-slate-50 rounded-lg">
                  <p>
                    <strong class="secondary-accent-text">Lookup:</strong>
                    Deterministic
                  </p>
                  <p>
                    <strong class="secondary-accent-text">Key:</strong>
                    Integer/String (Discrete)
                  </p>
                  <p>
                    <strong class="secondary-accent-text">Sparsity:</strong>
                    Inherent (By design)
                  </p>
                  <p>
                    <strong class="secondary-accent-text">Goal:</strong>
                    Minimize Latency
                  </p>
                </div>
              </div>
            </div>
          </section>

          <!-- Hardware Divide -->
          <section id="hardware" class="mb-16">
            <h2 class="text-3xl font-bold mb-6">The Hardware Dichotomy</h2>
            <p class="text-lg text-stone-600 mb-8">
              The two approaches are optimized for fundamentally different
              hardware. Understanding this is key to finding a path for
              integration.
            </p>
            <div class="grid md:grid-cols-2 gap-8 mb-8">
              <div class="card p-8 text-center">
                <div class="text-6xl mb-4">ðŸ§ </div>
                <h3 class="text-2xl font-bold mb-2">
                  CPU: The Latency Specialist
                </h3>
                <p>
                  Designed to execute single, complex tasks as fast as possible.
                  Excels at irregular logic and pointer-chasing, thanks to deep
                  caches and branch prediction. This is the world of Judy
                  arrays.
                </p>
              </div>
              <div class="card p-8 text-center">
                <div class="text-6xl mb-4">ðŸ’ª</div>
                <h3 class="text-2xl font-bold mb-2">
                  GPU: The Throughput General
                </h3>
                <p>
                  Designed to execute thousands of simple, parallel tasks
                  simultaneously. Excels at matrix math but is penalized by
                  divergent logic and random memory access. This is the world of
                  Attention.
                </p>
              </div>
            </div>
          </section>

          <!-- Architectural Proposals -->
          <section id="proposals" class="mb-16">
            <h2 class="text-3xl font-bold mb-6">
              Architectural Blueprints for a Hybrid Future
            </h2>
            <p class="text-lg text-stone-600 mb-8">
              A direct port is infeasible. Instead, we can apply Judy array
              *principles* to create novel hybrid architectures. Here are three
              strategic proposals.
            </p>

            <div class="space-y-12">
              <!-- Proposal 1 -->
              <article class="card p-8">
                <h3 class="text-2xl font-bold accent-text mb-2">
                  1. The Judy-Indexed KV Cache
                </h3>
                <p class="text-stone-600 mb-6">
                  This proposal targets the memory bottleneck in long-context
                  inference. It uses a CPU-side Judy array as a high-speed index
                  for the massive KV cache, which may be paged to system RAM.
                </p>
                <div class="flow-diagram">
                  <div class="flow-diagram-step" data-step="1">
                    <h4 class="font-semibold">Prefill & Index (CPU)</h4>
                    <p class="text-sm text-stone-600">
                      Key vectors from the prompt are hashed (via LSH) into
                      integer keys and stored in a Judy array on the CPU.
                    </p>
                  </div>
                  <div class="flow-diagram-step" data-step="2">
                    <h4 class="font-semibold">Query (GPU â†’ CPU)</h4>
                    <p class="text-sm text-stone-600">
                      During decoding, the new query vector is hashed and sent
                      to the CPU to perform a fast lookup in the Judy index.
                    </p>
                  </div>
                  <div class="flow-diagram-step" data-step="3">
                    <h4 class="font-semibold">Fetch & Attend (CPU â†’ GPU)</h4>
                    <p class="text-sm text-stone-600">
                      The CPU returns a small list of candidate indices. The GPU
                      fetches only these specific K-V pairs and performs
                      attention, avoiding a full cache scan.
                    </p>
                  </div>
                </div>
                <div class="mt-6 p-4 bg-amber-50 rounded-lg text-sm">
                  <p>
                    <strong class="accent-text">Benefit:</strong> Reduces
                    per-token attention complexity from O(N) to near O(log N),
                    enabling faster inference on much longer contexts.
                  </p>
                  <p>
                    <strong class="text-red-600">Challenge:</strong> CPU-GPU
                    communication overhead and potential accuracy loss from LSH
                    collisions.
                  </p>
                </div>
              </article>

              <!-- Proposal 2 -->
              <article class="card p-8">
                <h3 class="text-2xl font-bold accent-text mb-2">
                  2. Judy-Masked Attention
                </h3>
                <p class="text-stone-600 mb-6">
                  The most ambitious proposal: creating a GPU-native sparse
                  attention mechanism where a Judy-like structure represents the
                  attention mask itself with extreme memory efficiency.
                </p>
                <div class="flow-diagram">
                  <div class="flow-diagram-step" data-step="1">
                    <h4 class="font-semibold">Scout Kernel (GPU)</h4>
                    <p class="text-sm text-stone-600">
                      A lightweight kernel performs a low-cost, approximate
                      search to identify promising query-key pairs.
                    </p>
                  </div>
                  <div class="flow-diagram-step" data-step="2">
                    <h4 class="font-semibold">Mask Construction (GPU)</h4>
                    <p class="text-sm text-stone-600">
                      A second kernel takes these pairs and builds a GPU-native
                      `Judy1`-like sparse set (e.g., using bitmap nodes) in
                      shared memory.
                    </p>
                  </div>
                  <div class="flow-diagram-step" data-step="3">
                    <h4 class="font-semibold">Masked Attention (GPU)</h4>
                    <p class="text-sm text-stone-600">
                      The main attention kernel (e.g., FlashAttention) executes,
                      but only computes scores for pairs that exist in the
                      Judy-mask, skipping all others.
                    </p>
                  </div>
                </div>
                <div class="mt-6 p-4 bg-amber-50 rounded-lg text-sm">
                  <p>
                    <strong class="accent-text">Benefit:</strong> Enables fully
                    dynamic, data-dependent sparsity with memory usage
                    proportional to active connections, not sequence length.
                  </p>
                  <p>
                    <strong class="text-red-600">Challenge:</strong> Requires
                    designing a novel, high-throughput, parallel-construction
                    GPU radix/bitmap index from scratch.
                  </p>
                </div>
              </article>

              <!-- Proposal 3 -->
              <article class="card p-8">
                <h3 class="text-2xl font-bold accent-text mb-2">
                  3. Tree-based Positional Encodings (TPE)
                </h3>
                <p class="text-stone-600 mb-6">
                  This idea uses the radix trie structure of Judy arrays to
                  inspire a more structurally aware form of positional encoding,
                  based on a token's position in a vocabulary tree rather than a
                  linear sequence.
                </p>
                <div class="flow-diagram">
                  <div class="flow-diagram-step" data-step="1">
                    <h4 class="font-semibold">Vocabulary Trie</h4>
                    <p class="text-sm text-stone-600">
                      A static radix trie is constructed over the model's
                      vocabulary, grouping tokens with shared prefixes (e.g.,
                      "trans-form", "trans-former").
                    </p>
                  </div>
                  <div class="flow-diagram-step" data-step="2">
                    <h4 class="font-semibold">Path-based Encoding</h4>
                    <p class="text-sm text-stone-600">
                      A token's positional encoding is derived from its unique
                      path in this trie, capturing morphological relationships.
                    </p>
                  </div>
                  <div class="flow-diagram-step" data-step="3">
                    <h4 class="font-semibold">Structured Attention</h4>
                    <p class="text-sm text-stone-600">
                      The attention mechanism can now learn relationships based
                      on structural similarity (e.g., relating a word to its
                      plural) not just linear distance.
                    </p>
                  </div>
                </div>
                <div class="mt-6 p-4 bg-amber-50 rounded-lg text-sm">
                  <p>
                    <strong class="accent-text">Benefit:</strong> Provides a
                    powerful inductive bias for understanding language
                    structure, potentially improving sample efficiency.
                  </p>
                  <p>
                    <strong class="text-red-600">Challenge:</strong> The sheer
                    scale of modern vocabularies and the complexity of designing
                    a differentiable encoding scheme.
                  </p>
                </div>
              </article>
            </div>
          </section>

          <!-- Outlook -->
          <section id="outlook" class="mb-16">
            <h2 class="text-3xl font-bold mb-6">Strategic Outlook</h2>
            <p class="text-lg text-stone-600 mb-8">
              The path forward requires a multi-tiered strategy, balancing near-term wins with long-term research.
            </p>
            <div class="grid md:grid-cols-3 gap-8">
              <div class="card p-6">
                <div class="font-bold text-lg secondary-accent-text mb-2">Short-Term</div>
                <h4 class="font-bold text-xl mb-3">CPU as Co-processor</h4>
                <p class="text-sm text-stone-600">Focus on the Judy-Indexed KV Cache. Build proofs-of-concept for heterogeneous inference, optimizing data transfer and evaluating accuracy trade-offs.</p>
              </div>
              <div class="card p-6">
                <div class="font-bold text-lg secondary-accent-text mb-2">Mid-Term</div>
                <h4 class="font-bold text-xl mb-3">GPU-Native Sparse Sets</h4>
                <p class="text-sm text-stone-600">The critical research goal: develop a production-grade, CUDA-based library for sparse set representation inspired by Judy's bitmap nodes. This unlocks Judy-Masked Attention.</p>
              </div>
              <div class="card p-6">
                <div class="font-bold text-lg secondary-accent-text mb-2">Long-Term</div>
                <h4 class="font-bold text-xl mb-3">Architectural Exploration</h4>
                <p class="text-sm text-stone-600">Investigate fundamental shifts like Tree-based Positional Encodings and true hybrid CPU-GPU attention mechanisms, co-designing software and hardware for the next generation of AI.</p>
              </div>
            </div>
          </section>
        </main>

        <!-- AxWise Research Link -->
        <section class="bg-gradient-to-r from-blue-50 to-indigo-50 p-8 rounded-2xl border border-blue-200 mb-16">
          <h2 class="text-2xl font-bold text-blue-900 mb-4">Learn More About Our AI Research</h2>
          <p class="text-blue-700 leading-relaxed mb-6">
            This research is part of ongoing work at AxWise, where we explore the intersection of classical computer science
            and modern AI architectures. We focus on practical solutions that can bridge the gap between theoretical advances
            and real-world implementation challenges.
          </p>
          <a href="https://axwise.de"
             class="inline-flex items-center px-4 py-2 bg-blue-600 text-white font-medium rounded-lg hover:bg-blue-700 transition-colors"
             target="_blank" rel="noopener">
            Visit AxWise â†’
          </a>
        </section>
      </article>

      <!-- Footer -->
      <footer class="text-center pt-8 border-t border-stone-200">
        <p class="text-stone-500 mb-2">
          Â© 2025 AxWise. An exploration of hybrid AI architectures and data structure optimization.
        </p>
        <div class="flex justify-center space-x-4 text-sm">
          <a href="https://axwise.de" class="text-blue-600 hover:text-blue-800">AxWise Home</a>
          <span class="text-stone-400">â€¢</span>
          <a href="https://axwise.de/blog" class="text-blue-600 hover:text-blue-800">More Articles</a>
          <span class="text-stone-400">â€¢</span>
          <a href="https://axwise.de/research" class="text-blue-600 hover:text-blue-800">Research</a>
        </div>
      </footer>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const chartCanvas = document.getElementById('scaling-chart');
            const slider = document.getElementById('sequence-slider');
            const sliderValue = document.getElementById('slider-value');

            let chart;

            function createOrUpdateChart(n_value) {
                const labels = Array.from({ length: 20 }, (_, i) => Math.round(n_value / 20 * (i + 1)));
                const o_n_squared_data = labels.map(n => Math.pow(n, 2));
                const o_n_log_n_data = labels.map(n => n * Math.log(n));

                if (chart) {
                    chart.data.labels = labels;
                    chart.data.datasets[0].data = o_n_squared_data;
                    chart.data.datasets[1].data = o_n_log_n_data;
                    chart.update();
                    return;
                }

                const ctx = chartCanvas.getContext('2d');
                chart = new Chart(ctx, {
                    type: 'line',
                    data: {
                        labels: labels,
                        datasets: [{
                            label: 'O(N^2) Cost (Standard Attention)',
                            data: o_n_squared_data,
                            borderColor: '#d45087',
                            backgroundColor: 'rgba(212, 80, 135, 0.1)',
                            fill: true,
                            tension: 0.1
                        }, {
                            label: 'O(N log N) Cost (Efficient Attention)',
                            data: o_n_log_n_data,
                            borderColor: '#665191',
                            backgroundColor: 'rgba(102, 81, 145, 0.1)',
                            fill: true,
                            tension: 0.1
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        interaction: {
                            intersect: false,
                            mode: 'index',
                        },
                        scales: {
                            y: {
                                type: 'logarithmic',
                                title: {
                                    display: true,
                                    text: 'Computational Cost (Log Scale)'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'Sequence Length (N)'
                                }
                            }
                        },
                        plugins: {
                            tooltip: {
                                callbacks: {
                                    title: function(tooltipItems) {
                                        const item = tooltipItems[0];
                                        let label = item.chart.data.labels[item.dataIndex];
                                        return 'N = ' + (Array.isArray(label) ? label.join(' ') : label);
                                    },
                                    label: function(context) {
                                        let label = context.dataset.label || '';
                                        if (label) {
                                            label += ': ';
                                        }
                                        if (context.parsed.y !== null) {
                                            label += context.parsed.y.toExponential(2);
                                        }
                                        return label;
                                    }
                                }
                            }
                        }
                    }
                });
            }

            slider.addEventListener('input', (event) => {
                const value = parseInt(event.target.value);
                sliderValue.textContent = value.toLocaleString();
                createOrUpdateChart(value);
            });

            // Initial chart creation
            createOrUpdateChart(parseInt(slider.value));

            // Smooth scrolling for nav links
            document.querySelectorAll('a[href^="#"]').forEach(anchor => {
                anchor.addEventListener('click', function (e) {
                    e.preventDefault();
                    document.querySelector(this.getAttribute('href')).scrollIntoView({
                        behavior: 'smooth'
                    });
                });
            });
        });
    </script>

</body>
</html>
